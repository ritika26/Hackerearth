{"cells":[{"metadata":{"_kg_hide-input":true,"_uuid":"6f4b325135a4279f510a7bb9b31ed81982750946","trusted":true,"collapsed":true},"cell_type":"code","source":"## Import the required python utilities\nfrom plotly.offline import init_notebook_mode, iplot\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nimport pandas as pd\nimport numpy as np\n\n## Import sklearn important modules\nfrom sklearn.decomposition import PCA, SparsePCA, MiniBatchSparsePCA, KernelPCA, IncrementalPCA\nfrom sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection\nfrom sklearn.decomposition import TruncatedSVD, FastICA, NMF, FactorAnalysis\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.manifold import TSNE\nimport lightgbm as lgb\n\ninit_notebook_mode(connected=True)\npath = \"../input/\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d0145c4704c39f2e87804da5e6ff77da8974217"},"cell_type":"markdown","source":"## Dataset Decomposition Techniques  \n\n### Problem Statement: Santander Value Prediction\n\nSantander Group wants to identify the value of transactions for each potential customer. This is a first step that Santander needs to nail in order to personalize their services at scale. The dataset can be downloaded from this [link](https://www.kaggle.com/c/santander-value-prediction-challenge/data). In this kernel I have explained different approaches for dataset decomposition. \n\n\n### Introduction\n\n\nThe purpose of this kernel is to walkthrough different dataset decomposition techniques and their implementations.   Decomposition of dataset into lower dimensions often becomes an important task while deailing with datasets having larger number of features. Dimensionality Reduction refers to the process of converting a dataset having vast dimensions into a dataset with lesser number of dimensions. This process is done by ensuring that the information conveyed by the original dataset is not lost. \n\nCredits:\n- https://www.kaggle.com/arthurtok/interactive-intro-to-dimensionality-reduction\n\n### Contents\n\n1. Dataset Preparation    \n2. Feature Statistics    \n3. Eigen Values and Eigen Vectors   \n4. Principal Components Analysis   \n&nbsp;&nbsp;&nbsp;&nbsp; 4.1 Finding Right Number of Components   \n&nbsp;&nbsp;&nbsp;&nbsp; 4.2 PCA Implementation    \n&nbsp;&nbsp;&nbsp;&nbsp; 4.3 Variants of PCA  \n5. Truncated SVD   \n6. Fast ICA   \n7. Factor Analysis   \n8. Non-Negative Matrix Factorization  \n9. Gaussian Random Projection  \n10. Sparse Random Projection  \n11. tSNE Visualization  \n12. Baseline Model with Decomposition Features  "},{"metadata":{"_uuid":"22d7a3a9bf793d0b701881d9917d270541606584"},"cell_type":"markdown","source":"### 1. Dataset Preparation  \n\nAs the first step, load the required dataset. Also separate out the target variable and remove it from the original dataset. This step is done so that entire dataframe can be used directly in decomposition. "},{"metadata":{"_uuid":"623401ad5cfcf7f25cc18e77111538f36e1374da","trusted":true,"collapsed":true},"cell_type":"code","source":"train = pd.read_csv(path+'train.csv')\n\ntarget = train['target']\ntrain = train.drop([\"target\", \"ID\"], axis=1)\n\nprint (\"Rows: \" + str(train.shape[0]) + \", Columns: \" + str(train.shape[1]))\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fbd96d82447880ed508c1f1d985bb1c7916f7e30"},"cell_type":"markdown","source":"There are 4459 rows and 4992 features in the dataset which means that this dataset consists of more number of columns than number of rows. Normalize the dataset so that every value is in the same range "},{"metadata":{"_uuid":"051d63d124c1a87f3f45fb494c4fc1b6e6a7e12b","trusted":true,"collapsed":true},"cell_type":"code","source":"standardized_train = StandardScaler().fit_transform(train.values)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d0cbeb9878da3b8fb1d8670b9ea4748512a0097"},"cell_type":"markdown","source":"### 2. Feature Statistics  \n\nComputing the basic statistics about features such as mean, variance, standard deviation can help to understand about features. In this part, we will compute the following details about the features. "},{"metadata":{"_uuid":"94e1e18f248953a935a2670df212a4d3d99185aa","trusted":false,"collapsed":true},"cell_type":"code","source":"feature_df = train.describe().T\nfeature_df = feature_df.reset_index().rename(columns = {'index' : 'columns'})\nfeature_df['distinct_vals'] = feature_df['columns'].apply(lambda x : len(train[x].value_counts()))\nfeature_df['column_var'] = feature_df['columns'].apply(lambda x : np.var(train[x]))\nfeature_df['column_std'] = feature_df['columns'].apply(lambda x : np.std(train[x]))\nfeature_df['column_mean'] = feature_df['columns'].apply(lambda x : np.mean(train[x]))\nfeature_df['target_corr'] = feature_df['columns'].apply(lambda x : np.corrcoef(target, train[x])[0][1])\nfeature_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02939897996b684875bf240bb31b68830f78d6a6"},"cell_type":"markdown","source":"#### Variable Variance \n\nVariance defines how the data is spread across the mean. It is calulcated by taking the square of difference of every value from the mean value for a variable. One of the statistical intution is that if the feature variance is very less, then the feature will add less contribution to the model. However, I donot follow this blindly as most of the deep learning and boosting modles are robust to such issues. But, variance can give an idea about the features which can be discarded. Atleast the features having zero variance can be discarded because they are essently constant features. (again, these features might not be significant when considered individually, but can be useful in the row wise aggregated features) "},{"metadata":{"_uuid":"3576d69a7411586258902f3fb35c209462cf0c14","trusted":false,"collapsed":true},"cell_type":"code","source":"len(feature_df[feature_df['column_var'].astype(float) == 0.0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8bf65247401b7b21b2fd921c09a4ae97f4b3501a"},"cell_type":"markdown","source":"So there are 256 columns in the dataset having zero variance ie. they have constant values. \n\nLets plot the variance of the variables. "},{"metadata":{"_uuid":"608dd34c162e31ad46aa9f9c1f3da638fa3b8b07","trusted":false,"collapsed":true},"cell_type":"code","source":"feature_df = feature_df.sort_values('column_var', ascending = True)\nfeature_df['column_var'] = (feature_df['column_var'] - feature_df['column_var'].min()) / (feature_df['column_var'].max() - feature_df['column_var'].min())\ntrace1 = go.Scatter(x=feature_df['columns'], y=feature_df['column_var'], opacity=0.75, marker=dict(color=\"red\"))\nlayout = dict(height=400, title='Feature Variance', legend=dict(orientation=\"h\"));\nfig = go.Figure(data=[trace1], layout=layout);\niplot(fig);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2bcab853176e664f3f7bffe03154082d5e7f45ac","trusted":false,"collapsed":true},"cell_type":"code","source":"trace1 = go.Histogram(x=feature_df[feature_df['column_var'] <= 0.01]['column_var'], opacity=0.45, marker=dict(color=\"red\"))\nlayout = dict(height=400, title='Distribution of Variable Variance <= 0.01', legend=dict(orientation=\"h\"));\nfig = go.Figure(data=[trace1], layout=layout);\niplot(fig);\n\ntrace1 = go.Histogram(x=feature_df[feature_df['column_var'] > 0.01]['column_var'], opacity=0.45, marker=dict(color=\"red\"))\nlayout = dict(height=400, title='Distribution of Variable Variance > 0.01', legend=dict(orientation=\"h\"));\nfig = go.Figure(data=[trace1], layout=layout);\niplot(fig);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fbc4431775c191304655e34dfea5e494983b39af"},"cell_type":"markdown","source":"So we can see that a large number of variables has variable less than 0.01 and fewer variables has variance greater than 0.01 .\n\n#### Correlation with Target Variable \n\nPearson’s correlation coefficient is the test statistics that measures the statistical relationship, or association, between two continuous variables. \n\nAnother statistics test which which can be helpful about the columns is the correlation of the feature with the target variable. High correlated features are good for models for the reverse may not be true. Lets look at what is the distribution of correlations with the target variable in this dataset. "},{"metadata":{"_uuid":"eeeaaca0adaf1f38bb96244d7e0dca97ca18a165","trusted":false,"collapsed":true},"cell_type":"code","source":"trace1 = go.Histogram(x=feature_df['target_corr'], opacity=0.45, marker=dict(color=\"green\"))\nlayout = dict(height=400, title='Distribution of correlation with target', legend=dict(orientation=\"h\"));\nfig = go.Figure(data=[trace1], layout=layout);\niplot(fig);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e562d7b28c98e2db14c7c45c2963b79ee2f04f4"},"cell_type":"markdown","source":"As we can see that most of the variables are not very highly correlated with the target variable, and a majority of the variable have exteremely low correlation with the target. \n\nSo use of basic statistics is one of the method through which one can get idea about features having statistical significance. The same can be used to handpick important features and discard others.  \n\n### 3. Decomposition into EigenVectors and EigenValues  \n\nIn linear algebra, an eigenvector of a linear transformation is a non-zero vector that changes by only a scalar factor when that linear transformation is applied to it. If T is a linear transformation from a vector space V over a field F into itself and v is a vector in V that is not the zero vector, then v is an eigenvector of T if T(v) is a scalar multiple of v. \n\n> T(v)= λv\n\n\nwhere λ is a scalar value known as the eigenvalue or characteristic root associated with the eigenvector v. In terms of decomposition, eigen vectors are the principal components for any dataset. Lets visualize the individual and cumulative variance explained by eigen vectors. "},{"metadata":{"_uuid":"92d9ce0b564bc32b56fb720a8e38af6f278c2516","collapsed":true,"trusted":false},"cell_type":"code","source":"# Calculating Eigenvectors and eigenvalues of Cov matirx\nmean_vec = np.mean(standardized_train, axis=0)\ncov_matrix = np.cov(standardized_train.T)\neig_vals, eig_vecs = np.linalg.eig(cov_matrix)\n\n# Create a list of (eigenvalue, eigenvector) tuples\neig_pairs = [ (np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))]\n\n# Sort the eigenvalue, eigenvector pair from high to low\neig_pairs.sort(key = lambda x: x[0], reverse= True)\n\n# Calculation of Explained Variance from the eigenvalues\ntot = sum(eig_vals)\n\n# Individual explained variance\nvar_exp = [(i/tot)*100 for i in sorted(eig_vals, reverse=True)] \nvar_exp_real = [v.real for v in var_exp]\n\n# Cumulative explained variance\ncum_var_exp = np.cumsum(var_exp) \ncum_exp_real = [v.real for v in cum_var_exp]\n\n## plot the variance and cumulative variance \ntrace1 = go.Scatter(x=train.columns, y=var_exp_real, name=\"Individual Variance\", opacity=0.75, marker=dict(color=\"red\"))\ntrace2 = go.Scatter(x=train.columns, y=cum_exp_real, name=\"Cumulative Variance\", opacity=0.75, marker=dict(color=\"blue\"))\nlayout = dict(height=400, title='Variance Explained by Variables', legend=dict(orientation=\"h\", x=0, y=1.2));\nfig = go.Figure(data=[trace1, trace2], layout=layout);\niplot(fig);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e4667a83f3703f9735d7679a979a775c22b79b0"},"cell_type":"markdown","source":"So from the above graph we can observe the following points: About 500 features of this dataset can describe about 75% of the explained variance, About 1000 features of this dataset can describe about 90% of the explained variance and About 1500 features of this dataset can describe about 95% of the explained variance   \n\n\n### 4. PCA - Principal Component Analysis    \n\nPrincipal Component Analysis is the technique for finding most informative vectors of a high-dimensional datasets. In other words, PCA extracts the important variables in form of components from a datasets containing large number of features. The important features are extracted with the goal to capture maximum possible information from the dataset.  \n\nThe first principal component is a linear combination of dataset features having maximum variance. It determines the direction of highest variability in the data. If the components are uncorrelated, their directions should be orthogonal. This suggests the correlation b/w the components in zero. All succeeding principal component follows the similar concept i.e. they capture the remaining variation without being correlated with the previous component. \n\n\n### 4.1 Finding Right Number of Components\n\nWe observed from the cumulative frequency graph of eigen vectors that about 1000 variables can give upto 90% explained variance of the dataset. We can use PCA with N number of components and obtain the right number which matches a threshold value of explained variance. "},{"metadata":{"_uuid":"eaf5eeb25b9687b40b41054fbc2adce1fc6d4056","collapsed":true,"trusted":false},"cell_type":"code","source":"def _get_number_components(model, threshold):\n    component_variance = model.explained_variance_ratio_\n    explained_variance = 0.0\n    components = 0\n\n    for var in component_variance:\n        explained_variance += var\n        components += 1\n        if(explained_variance >= threshold):\n            break\n    return components\n\n### Get the optimal number of components\npca = PCA()\ntrain_pca = pca.fit_transform(standardized_train)\ncomponents = _get_number_components(pca, threshold=0.85)\ncomponents","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02bcef852d35b51032d4a0c2b00cc6ab30b31770"},"cell_type":"markdown","source":"So, for a threshold value = 0.85, we can choose 993 components. These components will explain about 85% of the variance of the dataset"},{"metadata":{"_kg_hide-input":true,"_uuid":"f2d878409d21cf769d1c848536da128bfa61d587","trusted":false,"collapsed":true},"cell_type":"code","source":"def plot_3_components(x_trans, title):\n    trace = go.Scatter3d(x=x_trans[:,0], y=x_trans[:,1], z = x_trans[:,2],\n                          name = target, mode = 'markers', text = target, showlegend = False,\n                          marker = dict(size = 8, color=x_trans[:,1], \n                          line = dict(width = 1, color = '#f7f4f4'), opacity = 0.5))\n    layout = go.Layout(title = title, showlegend= True)\n    fig = dict(data=[trace], layout=layout)\n    iplot(fig)\n\ndef plot_2_components(x_trans, title):\n    trace = go.Scatter(x=x_trans[:,0], y=x_trans[:,1], name=target, mode='markers',\n        text = target, showlegend = False,\n        marker = dict(size = 8, color=x_trans[:,1], line = dict(width = 1, color = '#fefefe'), opacity = 0.7))\n    layout = go.Layout(title = title, hovermode= 'closest',\n        xaxis= dict(title= 'First Component',\n            ticklen = 5, zeroline= False, gridwidth= 2),\n        yaxis=dict(title= 'Second Component',\n            ticklen = 5, gridwidth = 2), showlegend= True)\n    fig = dict(data=[trace], layout=layout)\n    iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c14871bf86240e0ed063242c8b986abaeeb117e"},"cell_type":"markdown","source":"### 4.2 Implementing PCA\n\nLets implement the PCA and visualize the first three and two components. Sklearn provides a good implementation of PCA and its variants.  "},{"metadata":{"_uuid":"bb37b2ae857f29c97cef247b31f52323749dbf2b","trusted":false,"collapsed":true},"cell_type":"code","source":"### Implement PCA \nobj_pca = model = PCA(n_components = components)\nX_pca = obj_pca.fit_transform(standardized_train)\n\n## Visualize the Components \nplot_3_components(X_pca, 'PCA - First Three Component')\nplot_2_components(X_pca, 'PCA - First Two Components')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ba52c111816185823df6302f6ce64501d8ccdef"},"cell_type":"markdown","source":"We can observe one main property of the components that they are orthogonal to each other, which means that they are uncorrelated. \n\n### 4.3 PCA Variants \n\nSklearn provides different Variants of PCA which can be helpful as well. \n\n**4.3.1 Kernel PCA:** \n\nKernelPCA is an extension of PCA which achieves non-linear dimensionality reduction through the use of kernels. It has many applications including denoising, compression and structured prediction (kernel dependency estimation). \n\n**4.3.2 Incremental PCA:** \n\nIncremental PCA works similar to PCA but Depending on the size of the input data, incremental PCA is much more memory efficient. This technique allows for partial computations which almost exactly match the results of PCA while processing the data in a minibatch fashion.   \n\n**4.3.3 Sparse PCA:**   \n\nSparse PCA finds out the set of sparse components that can optimally reconstruct the data. The amount of sparseness is tunable parameter. \n\n**4.3.4 Mini Batch Sparse PCA:**    \n\nMini Batch Sparse PCA is similar to sparse PCA but it computes the components by taking mini batches at onces from the data. It is faster but less accurate. "},{"metadata":{"_uuid":"9e918ac63534e939858b6d90a2f366ac124ef1ca"},"cell_type":"markdown","source":"### 5. Truncated SVD\n\nThe Singular-Value Decomposition or SVD is a matrix decomposition method for reducing a matrix to its constituent parts in order to make certain subsequent matrix calculations simpler. Truncated SVD is the variant of SVD which is also used for dimentionality reduction. Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work very well with sparse matrices efficiently. "},{"metadata":{"_uuid":"db406bdff6c3123be5bf04c7e925a321d0a14171","trusted":false,"collapsed":true},"cell_type":"code","source":"### Implement Truncated SVD \nobj_svd = TruncatedSVD(n_components = components)\nX_svd = obj_svd.fit_transform(standardized_train)\n\n## Visualize the Components \nplot_3_components(X_svd, 'Truncated SVD - First three components')\nplot_2_components(X_svd, 'Truncated SVD - First two components')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57b74862a752a171f06cdbc8e440fbd6cad1585b"},"cell_type":"markdown","source":"We can observe that results of PCA and Truncated SVD are pretty much similar. This is because PCA is (truncated) SVD on centered data (by per-feature mean substraction). If the data is already centered, those two classes will do the same, which we can observe in the graphs. TruncatedSVD is very useful on large sparse datasets which cannot be centered without making use of too much of memory.\n\n### 6. Independent Component Analysis - ICA\n\nIndependent component analysis separates the dataset containing multivariate features into additive subcomponents that are maximally independent. Typically, ICA is not used for dimentionality reduction but for separating the individual components. "},{"metadata":{"_uuid":"faab6278007d44871952261f5323426ef3e504b3","collapsed":true,"trusted":false},"cell_type":"code","source":"### Implement ICA \nobj_ica = FastICA(n_components = 30)\nX_ica = obj_ica.fit_transform(standardized_train)\n\n## Visualize the Components \nplot_3_components(X_ica, 'ICA - First three components')\nplot_2_components(X_ica, 'ICA - First two components')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"561ea5627103ec26b7db2f2d1097fa00d6014354"},"cell_type":"markdown","source":"### 7. Factor Analysis \n\nA simple linear generative model with Gaussian latent variables."},{"metadata":{"_uuid":"264e852cdd59009beee8c30ab7e10067fcca0db0","collapsed":true,"trusted":false},"cell_type":"code","source":"### Implement Factor Analysis \nobj_fa = FactorAnalysis(n_components = 30)\nX_fa = obj_fa.fit_transform(standardized_train)\n\n## Visualize the Components \nplot_3_components(X_fa, 'Factor Analysis - First three components')\n# plot_2_components(X, 'Factor Analysis - First two components')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10202b3b5b78ef648c27c53150f912efd5795579"},"cell_type":"markdown","source":"### 8. Non Negative Matrix Factorization\n\nNNMF is the technique which is used to find two non-negative matrices (W, H) whose product approximates the non- negative matrix X. "},{"metadata":{"_uuid":"39e416cdecab2be249915b0c1303a214f5e894dc","collapsed":true,"trusted":false},"cell_type":"code","source":"### Implement NonNegative Matrix Factorization\nobj = NMF(n_components = 2)\nX_nmf = obj.fit_transform(train)\n\n## Visualize the Components \n# plot_3_components(X, 'NNMF - First three components')\nplot_2_components(X_nmf, 'NNMF - First two components')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dab53d40ef2bf5ef30fb4cd9163bca8196619365"},"cell_type":"markdown","source":"### 9. Gaussian Random Projection  \n"},{"metadata":{"_uuid":"2475eb719d46dc2210a305b7e00accbdfe8daba5","collapsed":true,"trusted":false},"cell_type":"code","source":"### Implement Gaussian Random Projection\nobj_grp = GaussianRandomProjection(n_components = 30, eps=0.1)\nX_grp = obj_grp.fit_transform(standardized_train)\n\n## Visualize the Components \nplot_3_components(X_grp, 'Gaussian Random Projection - First three components')\nplot_2_components(X_grp, 'Gaussian Random Projection - First two components')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5076efbe30df1d3b6162175590fe243b8b7b87e1"},"cell_type":"markdown","source":"### 10. Sparse Random Projection  "},{"metadata":{"_uuid":"4810b0fdf014563cd686d816c085ab4a7c9b70a3","collapsed":true,"trusted":false},"cell_type":"code","source":"### Implement Sparse Random Projection\nobj_srp = SparseRandomProjection(n_components = 30, eps=0.1)\nX_srp = obj_srp.fit_transform(standardized_train)\n\n## Visualize the Components \nplot_3_components(X_srp, 'Sparse Random Projection - First three components')\nplot_2_components(X_srp, 'Sparse Random Projection - First two components')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7b5e3ac1271d43557f0d538117c55acd201285f"},"cell_type":"markdown","source":"### 11. Lets further decompose the dataset into two components:  t- SNE \n\nt-SNE was introduced in 2008 as the method for dataset decomposition using non-linear relations.  (t-SNE) t-Distributed Stochastic Neighbor Embedding is a non-linear dimensionality reduction algorithm used for exploring high-dimensional data. It maps multi-dimensional data to two or more dimensions suitable for human observation.  t-SNE is based on probability distributions with random walk on neighborhood graphs to find the structure within the data. The goal is to take a set of points in a high-dimensional space and find a representation of those points in a lower-dimensional space, typically the 2D plane. Lets apply it on the truncated svd components and further decompose the data into two components.\n\n"},{"metadata":{"_uuid":"d0f771e0b7875b4915aeb61436fe70582d69f353","collapsed":true,"trusted":false},"cell_type":"code","source":"tsne_model = TSNE(n_components=2, verbose=1, random_state=42, n_iter=500)\ntsne_results = tsne_model.fit_transform(X_svd)\n\ntraceTSNE = go.Scatter(\n    x = tsne_results[:,0],\n    y = tsne_results[:,1],\n    name = target,\n     hoveron = target,\n    mode = 'markers',\n    text = target,\n    showlegend = True,\n    marker = dict(\n        size = 8,\n        color = '#c94ff2',\n        showscale = False,\n        line = dict(\n            width = 2,\n            color = 'rgb(255, 255, 255)'\n        ),\n        opacity = 0.8\n    )\n)\ndata = [traceTSNE]\n\nlayout = dict(title = 'TSNE (T-Distributed Stochastic Neighbour Embedding)',\n              hovermode= 'closest',\n              yaxis = dict(zeroline = False),\n              xaxis = dict(zeroline = False),\n              showlegend= False,\n\n             )\n\nfig = dict(data=data, layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02fec0e9b0ae621b0ebadc2845a235488364b2b6"},"cell_type":"markdown","source":"### 12. Baseline Model with Decomposed Features\n\nLets create the baseline models using the decomposed features"},{"metadata":{"_uuid":"366a9a29f812b7b1f5bf1f791a49dbc9eb2f8a75","collapsed":true,"trusted":false},"cell_type":"code","source":"## add the decomposed features in the train dataset\ndef _add_decomposition(df, decomp, ncomp, flag):\n    for i in range(1, ncomp+1):\n        df[flag+\"_\"+str(i)] = decomp[:, i - 1]\n\n_add_decomposition(train, X_pca, 30, 'pca')\n_add_decomposition(train, X_svd, 30, 'svd')\n_add_decomposition(train, X_ica, 30, 'ica')\n_add_decomposition(train, X_fa, 30, 'fa')\n_add_decomposition(train, X_grp, 30, 'grp')\n_add_decomposition(train, X_srp, 30, 'srp')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea1e7e54eba10572b1867fdacfd18bbffcfa476a"},"cell_type":"markdown","source":"Prepare the dataset - Create train, test splits and obtain the feature names"},{"metadata":{"_uuid":"9c7cb691d8dc5e2125f0bc68f4e35e18e6201b45","collapsed":true,"trusted":false},"cell_type":"code","source":"## create the lists of decomposed and non decomposed features \nall_features = [x for x in train.columns if x not in [\"ID\", \"target\"]]\nall_features = [x for x in all_features if \"_\" not in x]\ndecomposed_features = [x for x in train.columns if \"_\" in x]\n\n## split the dataset into train test validation\ntarget_log = np.log1p(target.values)\ntrain_x, val_x, train_y, val_y = train_test_split(train, target_log, test_size=0.20, random_state=2018)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed794b9a5bd3cd1c1b76ac4648638493b33abe88"},"cell_type":"markdown","source":"Train the lightgbm model without decomposed features."},{"metadata":{"_uuid":"ae1197dfcc45cb8d7e320acc95eac832d9b09680","collapsed":true,"trusted":false},"cell_type":"code","source":"## create a baseline model with all features \nparams = {'learning_rate': 0.01, \n          'max_depth': 16, \n          'boosting': 'gbdt', \n          'objective': 'regression', \n          'metric': 'rmse', \n          'is_training_metric': True, \n          'num_leaves': 144, \n          'feature_fraction': 0.9, \n          'bagging_fraction': 0.7, \n          'bagging_freq': 5, \n          'seed':2018}\n\n## model without decomposed features \ntrain_X = lgb.Dataset(train_x[all_features], label=train_y)\nval_X = lgb.Dataset(val_x[all_features], label=val_y)\nmodel1 = lgb.train(params, train_X, 1000, val_X, verbose_eval=100, early_stopping_rounds=100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"45f5edb2812e8f5d023ae5c356d3e83697e37e68"},"cell_type":"markdown","source":"Lets now train another model which uses only decomposed features "},{"metadata":{"_uuid":"2bd4b358abf5f5cf8a334aef327583eb5c382739","collapsed":true,"trusted":false},"cell_type":"code","source":"## create a model with decomposed features \ntrain_X = lgb.Dataset(train_x[decomposed_features], label=train_y)\nval_X = lgb.Dataset(val_x[decomposed_features], label=val_y)\nmodel2 = lgb.train(params, train_X, 3000, val_X, verbose_eval=100, early_stopping_rounds=100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca41f1b36c53a2a0962829ce11ecf163341a44b2"},"cell_type":"markdown","source":"Lets use all the features in the model, but lets select the important features using random forests "},{"metadata":{"_uuid":"3b2440a83f8a09137fd1d0fad5cd25e17e786dcb","collapsed":true,"trusted":false},"cell_type":"code","source":"## Find important features using Random Forests \ncomplete_features = all_features + decomposed_features\nmodel = RandomForestRegressor(n_jobs=-1, random_state=2018)\nmodel.fit(train[complete_features], target)\nimportances = model.feature_importances_\n\n## get list of important features \nimportances_df = pd.DataFrame({'importance': importances, 'feature': complete_features})\nimportances_df = importances_df.sort_values(by=['importance'], ascending=[False])\nimportant_features = importances_df[:750]['feature'].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb60068f2894a260b6f6dacb388e9eefccbb9ec7"},"cell_type":"markdown","source":"Create the model with important features "},{"metadata":{"_uuid":"772536bafbf3c8ff6064b48a456fa7c0b1c202d6","collapsed":true,"trusted":false},"cell_type":"code","source":"## create a model with important features   \ntrain_X = lgb.Dataset(train_x[important_features], label=train_y)  \nval_X = lgb.Dataset(val_x[important_features], label=val_y)  \nmodel3 = lgb.train(params, train_X, 3000, val_X, verbose_eval=100, early_stopping_rounds=100)  ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3a04ad2bb6a642c5f7d9ad0fa79c9c31ffda88b"},"cell_type":"markdown","source":"Predit the output on test data"},{"metadata":{"_uuid":"efa43f21aaf85dca1783dd3354975a8191f81787","collapsed":true,"trusted":false},"cell_type":"code","source":"test = pd.read_csv(path+\"test.csv\")\ntestid = test.ID.values\ntest = test.drop('ID', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2c181bfe5909e9893743917769381fffddaf20"},"cell_type":"markdown","source":"Obtain the decomposed components for test data"},{"metadata":{"_uuid":"e80ac9c8e883bb84771af8237ed06cd152b585ae","collapsed":true,"trusted":false},"cell_type":"code","source":"## obtain the components from test data\nstandardized_test = StandardScaler().fit_transform(test[all_features].values)\ntsX_pca = obj_pca.transform(standardized_test)\ntsX_svd = obj_svd.transform(standardized_test)\ntsX_ica = obj_ica.transform(standardized_test)\ntsX_fa  = obj_fa.transform(standardized_test)\ntsX_grp = obj_grp.transform(standardized_test)\ntsX_srp = obj_srp.transform(standardized_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"49d61639b433af867c96687d36e659e8c83c5fd9"},"cell_type":"markdown","source":"Add the components to test data"},{"metadata":{"_uuid":"f7eebe0989201e2748b2738a3237cffede87385a","collapsed":true,"trusted":false},"cell_type":"code","source":"## add the components in test data\n_add_decomposition(test, tsX_pca, 30, 'pca')\n_add_decomposition(test, tsX_svd, 30, 'svd')\n_add_decomposition(test, tsX_ica, 30, 'ica')\n_add_decomposition(test, tsX_fa, 30, 'fa')\n_add_decomposition(test, tsX_grp, 30, 'grp')\n_add_decomposition(test, tsX_srp, 30, 'srp')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ac3d50617e134fc7a03085fc2ce26aee38d1900"},"cell_type":"markdown","source":"Predict the output"},{"metadata":{"_uuid":"25efa37c3742d73a04bcc7b48e71ceed06883e6c","collapsed":true,"trusted":false},"cell_type":"code","source":"## create submission file \npred = np.expm1(model3.predict(test[important_features], num_iteration=model3.best_iteration))\nsub = pd.DataFrame()\nsub['ID'] = testid\nsub['target'] = pred\nsub.to_csv('submission.csv', index=False)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f642b65fa23e3c71d5ad158cd0a44fb7b1ddd423"},"cell_type":"markdown","source":"This is the baseline model. To improve this one can perform the following ideas: \n    \n- extensive feature engineering: aggregated features, feature group statistics, mini batch kmeans clustering etc.  \n- models fine tuning \n- stacking / ensembling  \n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"af7dbf3ce1dddb51c5432429dd8deb3435165fa2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}