{"cells":[{"metadata":{"_uuid":"3b11d15b4ca748f6ac9978268814b1539e3493f5","extensions":{"jupyter_dashboards":{"version":1,"views":{"report_default":{},"grid_default":{"height":4,"col":0,"hidden":false,"width":4,"row":0}}}},"_cell_guid":"69ca7bb7-db09-4e5a-8ccd-1880cbc00d19"},"cell_type":"markdown","source":"## Introduction \n\nThis is a feature engineering notebook for the DonorsChoose.org Application Screening competition. The objective is to predict whether teachers' project proposals are accepted or rejected. In this notebook, I have described different types of features that can be engineered with the given dataset. These features can be used in the classification models.  \n\n### Contents\n\n1. Aggregated Features\n2. Date-Time Features\n3. Text Based Features\n4. NLP Based Features\n5. TF-IDF Features\n    - Word Level TF-IDF\n    - Character Level TF-IDF\n6. Word Embedding Features\n7. Topic Modelling Features\n8. Count Features"},{"metadata":{"_uuid":"d1e3afef50f6715005e999fc84cedb3884f2db49","collapsed":true,"_cell_guid":"6cd596f7-3eb9-4f1a-af8d-262c1309ee46","trusted":false},"cell_type":"code","source":"# !! Important !!\n# Set small_run = False, to run this feature engineering notebook for entire dataframe\n# Setting small_run = True, runs the notebook only for top 100 rows of the dataframe\n# I have added this flag so that this notebook can be executed in kaggle kernal\n\nrun_for_small_data = True","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8efb5a4c799489956abbae649c9cfde2a51235d7","collapsed":true,"_cell_guid":"6e0cf2fc-ca16-49fd-bdcc-ac8937708f0b","trusted":false},"cell_type":"code","source":"# import the required libraries \n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing import sequence, text\nfrom keras.layers import Input, Embedding\n\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom textblob import TextBlob\n\nimport datetime as dt\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport string\n\n# stop_words = []\nstop_words = list(set(stopwords.words('english')))\nwarnings.filterwarnings('ignore')\npunctuation = string.punctuation","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c04395458a77e8df01fa893cd2c8fc1a3ab564d","collapsed":true,"extensions":{"jupyter_dashboards":{"version":1,"views":{"report_default":{},"grid_default":{"hidden":true}}}},"_cell_guid":"fb90ca33-d935-4760-93bc-0c52c5097990","trusted":false},"cell_type":"code","source":"# read data files \n\nid_column = \"id\"\nmissing_token = \" UNK \"\n\ntrain = pd.read_csv(\"../input/donorschoose-application-screening/train.csv\", parse_dates=[\"project_submitted_datetime\"])\ntest = pd.read_csv(\"../input/donorschoose-application-screening/test.csv\", parse_dates=[\"project_submitted_datetime\"])\nrc = pd.read_csv(\"../input/donorschoose-application-screening/resources.csv\").fillna(missing_token)\n\ndf = pd.concat([train, test], axis=0) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"101b90a18606b7469b4c0e7ef8a2e85294dfab7a","_cell_guid":"3a506205-d0e9-4e84-8801-b5af410536a7"},"cell_type":"markdown","source":"### 1. Aggregated Features\n\nFeatures obtained by aggregating the fields from resources data and the training data\n\n- **Feature 1,2,3 - Min Price, Max Price, Mean Price**: Min, Max, and Mean value of Price of resources requested.\n\n- **Feature 4,5,6 - Min Quantity, Max Quantity, Mean Quantity**: Min, Max, and Mean value of Quantity of resources requested.\n\n- **Feature 7,8,9 - Min Total Price, Max Total Price, Mean Total Price**: Min, Max, and Mean value of Total Price of resources requested.\n\n- **Feature 10,11,12 - Sum of Total Price**: Total price of all the resoruces requested by the teacher in a proposal\n\n- **Feature 13 - Items Requested**: Total unique number of items requested by the teacher in a proposal\n\n- **Feature 14 - Quantity**: Total number of quantities requested by the teacher in a proposal"},{"metadata":{"_uuid":"0d924e6f83b0ae36483fcc56d7dd0372b071397b","collapsed":true,"extensions":{"jupyter_dashboards":{"version":1,"views":{"report_default":{},"grid_default":{"hidden":true}}}},"_cell_guid":"2d6b1a81-bde4-45b9-9f0c-ed890bcd6087","trusted":false},"cell_type":"code","source":"rc['total_price'] = rc['quantity']*rc['price']\nagg_rc = rc.groupby('id').agg({'description':'count', 'quantity':'sum', 'price':'sum', 'total_price':'sum'}).rename(columns={'description':'items'})\n\nfor func in ['min', 'max', 'mean']:\n    agg_rc_temp = rc.groupby('id').agg({'quantity':func, 'price':func, 'total_price':func}).rename(columns={'quantity':func+'_quantity', 'price':func+'_price', 'total_price':func+'_total_price'}).fillna(0)\n    agg_rc = agg_rc.join(agg_rc_temp)\n\nagg_rc = agg_rc.join(rc.groupby('id').agg({'description':lambda x:' '.join(x.values.astype(str))}).rename(columns={'description':'resource_description'}))\n\ndf = df.join(agg_rc, on='id')\n\nif run_for_small_data:\n    df = df.head(100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c46dff5acccb688ba42343dcaed9c0ec51229273","collapsed":true,"_cell_guid":"f40a691e-e335-463d-8d17-b35097ae7896","trusted":false},"cell_type":"code","source":"df[['price', 'total_price', 'items', 'quantity', 'min_price', 'min_total_price', 'min_quantity', \n    'max_price', 'max_total_price', 'max_quantity', 'mean_price', 'mean_total_price', 'mean_quantity']].head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9415da14ba2d49ee14d1e98f33d49655a40c71bf","_cell_guid":"5ae6b166-13b7-45c1-bb68-3ffd3f4eb46a"},"cell_type":"markdown","source":"\n### 2. Datetime Features \n\nFeatures extracted from project submitted datetime\n\n- **Feature 15 - Year of Submission**: Value of year when the proposal was submitted\n- **Feature 16 - Month of Submission**: Month number (values between 1 to 12) when the proposal was submitted\n- **Feature 17 - Week Day of Submission**: Week Day value (values between 1 to 7) when the proposal was submitted\n- **Feature 18 - Hour of Submission**: Value of time hour (values between 0 to 23) when the proposal was submitted\n- **Feature 19 - Year Day of Submission**: Year Day (values between 1 to 365) when the proposal was submitted\n- **Feature 20 - Month Day of Submission**: Month Day (values between 1 to 31) when the proposal was submitted\n\n"},{"metadata":{"_uuid":"711a020d70e80b5b2c56c58eaf1541b279f2f415","collapsed":true,"_cell_guid":"d47d30e0-f428-4d6f-900b-9d7abb1f15a7","trusted":false},"cell_type":"code","source":"# extracting datetime features using datetime module \ndf[\"Year\"] = df[\"project_submitted_datetime\"].dt.year\ndf[\"Month\"] = df[\"project_submitted_datetime\"].dt.month\ndf['Weekday'] = df['project_submitted_datetime'].dt.weekday\ndf[\"Hour\"] = df[\"project_submitted_datetime\"].dt.hour\ndf[\"Month_Day\"] = df['project_submitted_datetime'].dt.day\ndf[\"Year_Day\"] = df['project_submitted_datetime'].dt.dayofyear","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3667444ceb01b0f50400c87e38a917ceb2d6a911","collapsed":true,"_cell_guid":"4c09a6e6-dba2-46b0-8fc7-a9056916f735","trusted":false},"cell_type":"code","source":"df[['Year', 'Month', 'Weekday', 'Hour', 'Month_Day', 'Year_Day']].head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8329792e6815ededadb59e7ec0fa61c17777e75d","_cell_guid":"bc164020-fb25-421c-95e5-3ec7ff11d763"},"cell_type":"markdown","source":"\n### 3. Text based features \n\nFeatures extracted from proposal essay text and resources description\n\n- **Feature 21: Length of Essay 1** - total number of characters in essay 1 including spaces\n- **Feature 22: Length of Essay 2** - total number of characters in essay 2 including spaces\n- **Feature 23: Length of Essay 3** - total number of characters in essay 3 including spaces\n- **Feature 24: Length of Essay 4** - total number of characters in essay 4 including spaces\n- **Feature 25: Length of Project Title** - total number of characters in project title including spaces\n- **Feature 26: Word Count in the Complete Essay** - total number of words in the complete essay text\n- **Feature 27: Character Count in the Complete Essay** - total number of characters in complete essay text\n- **Feature 28: Word Density of the Complete Essay** - average length of the words used in the essay\n- **Feature 29: Puncutation Count in the Complete Essay** - total number of punctuation marks in the essay\n- **Feature 30: Upper Case Count in the Complete Essay** - total number of upper count words in the essay\n- **Feature 31: Title Word Count in the Complete Essay** - total number of proper case (title) words in the essay\n- **Feature 32: Stopword Count in the Complete Essay** - total number of stopwords in the essay\n"},{"metadata":{"_uuid":"c5f8d627d9ded66b9f28172b536a97a15adf0fcb","collapsed":true,"_cell_guid":"48a7e174-d282-430f-b920-7dd493729af3","trusted":false},"cell_type":"code","source":"# fillup empty values with missing token \ndf['project_essay_3'] = df['project_essay_3'].fillna(missing_token)\ndf['project_essay_4'] = df['project_essay_4'].fillna(missing_token)\n\n# extract length of each essay and title\ndf[\"essay1_len\"] = df['project_essay_1'].apply(len)\ndf[\"essay2_len\"] = df['project_essay_2'].apply(len)\ndf[\"essay3_len\"] = df['project_essay_3'].apply(len)\ndf[\"essay4_len\"] = df['project_essay_4'].apply(len)\ndf[\"title_len\"] = df['project_title'].apply(len)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2cc656313e15fd12ae0d7b7f16e08087f089549d","collapsed":true,"_cell_guid":"a7bffc75-24b8-4fb1-89e7-281eb946b3c8","trusted":false},"cell_type":"code","source":"df[['essay1_len', 'essay2_len', 'essay3_len', 'essay4_len', 'title_len']].head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d054c4b64b19e1337dc06fe6b4a19574a3c07bd","collapsed":true,"_cell_guid":"af3db852-56ac-42a0-973a-7885ed4028af","trusted":false},"cell_type":"code","source":"# combine the project essays to create a complete essay text\ndf['text'] = df.apply(lambda row: ' '.join([str(row['project_essay_1']), \n                                            str(row['project_essay_2']), \n                                            str(row['project_essay_3']), \n                                            str(row['project_essay_4'])]), axis=1)\n\n# extract features from text\ndf['char_count'] = df['text'].apply(len)\ndf['word_count'] = df['text'].apply(lambda x: len(x.split()))\ndf['word_density'] = df['char_count'] / (df['word_count']+1)\ndf['punctuation_count'] = df['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in punctuation))) \ndf['title_word_count'] = df['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\ndf['upper_case_word_count'] = df['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\ndf['stopword_count'] = df['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.lower() in stop_words]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ed5050c2f571882032500ffe4114ceb572c7926","collapsed":true,"_cell_guid":"b96775cd-9269-4957-a9e7-16ae6524a14f","trusted":false},"cell_type":"code","source":"df[['char_count', 'word_count', 'word_density', 'punctuation_count', 'title_word_count', 'upper_case_word_count', 'stopword_count']].head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"270cc2b7db4aad7b157a69cd1bb073529038a459","_cell_guid":"f551ac4e-db19-43e5-9541-ac9f9b325162"},"cell_type":"markdown","source":"\n### 4. More NLP based features \n\nPart of Speech and Sentiment related features from the text. I have used python's textblob package to get the sentiment related features and part-of-speech tags of the tokens in the sentence. \n\n- **Feature 33: Article Polarity** - total number of characters in essay 1 including spaces\n- **Feature 34: Article Subjectivity** - total number of characters in essay 2 including spaces\n- **Feature 35: Noun Count** - total number of characters in essay 3 including spaces\n- **Feature 36: Verb Count** - total number of characters in essay 4 including spaces\n- **Feature 37: Adjective Count** - total number of characters in project title including spaces\n- **Feature 38: Adverb Count** - total number of words in the complete essay text\n- **Feature 39: Pronoun Count** - total number of characters in complete essay text\n"},{"metadata":{"_uuid":"9fc0d183738ca4aa8b69101d742c6e49aed34f89","collapsed":true,"_cell_guid":"68e298ec-fbd9-42ef-8a2f-25a0204942fb","trusted":false},"cell_type":"code","source":"# functions to get polatiy and subjectivity of text using the module textblob\ndef get_polarity(text):\n    try:\n        textblob = TextBlob(unicode(text, 'utf-8'))\n        pol = textblob.sentiment.polarity\n    except:\n        pol = 0.0\n    return pol\n\ndef get_subjectivity(text):\n    try:\n        textblob = TextBlob(unicode(text, 'utf-8'))\n        subj = textblob.sentiment.subjectivity\n    except:\n        subj = 0.0\n    return subj\n\n\n# change df_small to df to create these features on complete dataframe\ndf['polarity'] = df['text'].apply(get_polarity)\ndf['subjectivity'] = df['text'].apply(get_subjectivity)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8fb2c52afa0264caa2dc51d30d7acfea0e5cf4a9","collapsed":true,"_cell_guid":"e03840c7-445f-4f9f-83b6-d4c336af4dd3","trusted":false},"cell_type":"code","source":"df[['polarity', 'subjectivity']].head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df3507b106805026f1d96d934b1c30cbf82ad5ab","collapsed":true,"_cell_guid":"0c252b52-887c-4f44-a9c0-700c3e57f535","trusted":false},"cell_type":"code","source":"pos_dic = {\n    'noun' : ['NN','NNS','NNP','NNPS'],\n    'pron' : ['PRP','PRP$','WP','WP$'],\n    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n    'adj' :  ['JJ','JJR','JJS'],\n    'adv' : ['RB','RBR','RBS','WRB']\n}\n\n# function to check and get the part of speech tag count of a words in a given sentence\ndef pos_check(x, flag):\n    cnt = 0\n    try:\n        wiki = TextBlob(x)\n        for tup in wiki.tags:\n            ppo = list(tup)[1]\n            if ppo in pos_dic[flag]:\n                cnt += 1\n    except:\n        pass\n    return cnt\n\ndf['noun_count'] = df['text'].apply(lambda x: pos_check(x, 'noun'))\ndf['verb_count'] = df['text'].apply(lambda x: pos_check(x, 'verb'))\ndf['adj_count'] = df['text'].apply(lambda x: pos_check(x, 'adj'))\ndf['adv_count'] = df['text'].apply(lambda x: pos_check(x, 'adv'))\ndf['pron_count'] = df['text'].apply(lambda x: pos_check(x, 'pron'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3c35f84fcee4470f1fc4a8b55dce7b26fd4f39b","collapsed":true,"_cell_guid":"37f28001-0a4d-43e9-9fc1-2ae64a718cc2","trusted":false},"cell_type":"code","source":"df[['noun_count', 'verb_count', 'adj_count', 'adv_count', 'pron_count']].head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3dc3102120233787c3f788c0032f3fe16bd7adc","_cell_guid":"efc8840f-bf01-4fe2-b84b-53eb37f8ed0d"},"cell_type":"markdown","source":"\n### 5. TF-IDF Features\n\nTf-idf weight is composed by two terms: the first computes the normalized Term Frequency (TF), the second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.\n\n- TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n- IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n\nReference: http://www.tfidf.com/\n\n\n\n- **Feature 40:** Word Level N-Gram TF-IDF of Article Text\n- **Feature 41:** Word Level N-Gram TF-IDF of Project Title\n- **Feature 42:** Word Level N-Gram TF-IDF of Resource Text\n- **Feature 43:** Character Level N-Gram TF-IDF of Article Text\n- **Feature 44:** Character Level N-Gram TF-IDF of Project Title\n- **Feature 45:** Character Level N-Gram TF-IDF of Resource Text\n"},{"metadata":{"_uuid":"fb00ad103ca14bb527ffeb943f7a167e200ac32d","collapsed":true,"extensions":{"jupyter_dashboards":{"version":1,"views":{"report_default":{},"grid_default":{"height":11,"col":4,"hidden":false,"width":4,"row":0}}}},"_cell_guid":"41fc7e2f-4dd7-42e3-98b4-231194fc19d2","trusted":false},"cell_type":"code","source":"df['article_text'] = df.apply(lambda row: ' '.join([str(row['project_essay_1']), str(row['project_essay_2']), \n                                         str(row['project_essay_3']), str(row['project_essay_4'])]), axis=1)\ndf['resource_text'] = df.apply(lambda row: ' '.join([str(row['resource_description']), str(row['project_resource_summary'])]), axis=1)\n\nresource_text = list(df['resource_text'].values)\ntitle_text = list(df['project_title'].values)\narticle_text = list(df['article_text'].values)\n\n# word level tf-idf for article text\nvect_word = TfidfVectorizer(max_features=2500, analyzer='word', stop_words='english', ngram_range=(1,3), dtype=np.float32) \nvect_word.fit(article_text)\ntfidf_complete = vect_word.transform(article_text)\n\n# word level tf-idf for project title\nvect_word = TfidfVectorizer(max_features=500, analyzer='word', stop_words='english', ngram_range=(1,3), dtype=np.float32) \nvect_word.fit(title_text)\ntfidf_title = vect_word.transform(title_text)\n\n# word level tf-idf for resource text\nvect_word = TfidfVectorizer(max_features=1000, analyzer='word', stop_words='english', ngram_range=(1,3), dtype=np.float32) \nvect_word.fit(resource_text)\ntfidf_resource = vect_word.transform(resource_text)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e1b0d7d5d1f7728bfc1e30d0d9f23e8c4b6611c4","collapsed":true,"_cell_guid":"81b85c19-d98d-4650-a331-dbb33a3f7d29","trusted":false},"cell_type":"code","source":"#  create a dictionary mapping the tokens to their tfidf values\ntfidf = dict(zip(vect_word.get_feature_names(), vect_word.idf_))\ntfidf = pd.DataFrame(columns=['title_word_tfidf']).from_dict(dict(tfidf), orient='index')\ntfidf.columns = ['title_word_tfidf']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e88a9de528986bfe4299586777981e3920a30b4","collapsed":true,"_cell_guid":"a02a231a-9014-4603-bc5f-d6d1a4f32426","trusted":false},"cell_type":"code","source":"# features with highest tf-idf (in title)\ntfidf.sort_values(by=['title_word_tfidf'], ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e91b399e8b595307911f6b9c45f4704b9a579283"},"cell_type":"markdown","source":"Similarly we can generate character level tf-idfs"},{"metadata":{"_uuid":"21f4462797cc609a061694b6d1de909aa42c03e5","collapsed":true,"_cell_guid":"96a4bb16-0ffa-4e30-a632-356a4d577c3f","trusted":false},"cell_type":"code","source":"# character level tf-idf for article text\nchar_word = TfidfVectorizer(max_features=2000, analyzer='char', stop_words='english', ngram_range=(1,3), dtype=np.float32) \nchar_word.fit(article_text)\ntfidf_complete_char = char_word.transform(article_text)\n\n# character level tf-idf for project title\nchar_word = TfidfVectorizer(max_features=500, analyzer='char', stop_words='english', ngram_range=(1,3), dtype=np.float32) \nchar_word.fit(title_text)\ntfidf_title_char = char_word.transform(title_text)\n\n# character level tf-idf for resource text\nchar_word = TfidfVectorizer(max_features=600, analyzer='char', stop_words='english', ngram_range=(1,3), dtype=np.float32) \nchar_word.fit(resource_text)\ntfidf_resource_char = char_word.transform(resource_text)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"381ca5a22d84ea4ce0ab19b58f856206b24ddfde","_cell_guid":"c588a133-cc85-4e46-ab87-437a71dcfd7f"},"cell_type":"markdown","source":"\n### 6. Word Embeddings\n\n**Feature 46:** WordEmbedding Vectors of text data\n\nWord Embedding Vectors can be trained itself using the corpus or they can be generated using Pre-Trained word embeddings. \n\nA word embedding is a class of approaches for representing words and documents using a dense vector representation. It is an improvement over more the traditional bag-of-word model encoding schemes where large sparse vectors were used to represent each word or to score each word within a vector to represent an entire vocabulary. These representations were sparse because the vocabularies were vast and a given word or document would be represented by a large vector comprised mostly of zero values. Instead, in an embedding, words are represented by dense vectors where a vector represents the projection of the word into a continuous vector space. The position of a word within the vector space is learned from text and is based on the words that surround the word when it is used.\n\nReference: https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n"},{"metadata":{"_uuid":"eb084d417cbe9f890a7181fe6f89d05171271432","collapsed":true,"_cell_guid":"e9fea656-2819-4eba-9827-43af7635271b","trusted":false},"cell_type":"code","source":"xtrain = df.text.values\n\n# load the pre-trained word-vectors\nembeddings_index = {}\n\nEMBEDDING_FILE = '../input/fatsttext-common-crawl/crawl-300d-2M/crawl-300d-2M.vec'\nf = open(EMBEDDING_FILE)\nfor line in f:\n    if run_for_small_data and len(embeddings_index) == 100:\n      break\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\n# perform pre-processing in keras\nmax_features = 100000 # max number of words to use in word embedding matrix\nmax_len = 300 # max length of the word embedding vector\n\n# Tokenization of text data\ntoken = text.Tokenizer(num_words=max_features)\ntoken.fit_on_texts(list(xtrain))\nword_index = token.word_index\n\n# Create sequence of Tokens and Pad them to create equal length vectors\nxtrain_seq = token.texts_to_sequences(xtrain)\nxtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n\n# Create an embedding matrix of words in the data\nembedding_matrix = np.zeros((len(word_index) + 1, 300))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d9a73e09fe46b8ca91d2ad2d29ff5d50703c35b1","_cell_guid":"4a95cbdc-6796-4463-b1b3-f6bc343ce7d6"},"cell_type":"markdown","source":"#### Using Word Embedding Features\n\n**Option 1:** Create Sentence Vectors\n\nThere are differet methods to get the sentence vectors :\n\n- Doc2Vec : Train your dataset using Doc2Vec and then use the sentence vectors.\n- Average of Word2Vec vectors : Take the average of all the word vectors in a sentence. This average vector will represent the sentence vector. In this notebook I have used this approach. \n- Average of Word2Vec vectors with TF-IDF : Take the word vectors, multiply it with their TF-IDF scores and take the average to get sentence vector.\n\n\n**Option 2:** Use Word Embeddings Directly\n\nKeras offers an Embedding layer that can be used for neural networks on text data. It requires that the input data be integer encoded, so that each word is represented by a unique integer. This data preparation step can be performed using the Tokenizer API also provided with Keras. The Embedding layer is initialized with random weights and will learn an embedding for all of the words in the training dataset."},{"metadata":{"_uuid":"97a1ba43a36184d4902a0724743630c23d0e9fdb","collapsed":true,"_cell_guid":"54fbabe8-e4d9-4f5f-9559-eed15f1a7536","trusted":false},"cell_type":"code","source":"# Option One: Create Sentence to vector\n    \n# function to generate sentence vector of the sentence\ndef sent2vec(sentence):\n    M = []\n    for w in word_tokenize(sentence):\n#     for w in word_tokenize(unicode(sentence, 'utf8')):\n        if not w.isalpha():\n            continue\n        if w in embeddings_index:\n            M.append(embeddings_index[w])\n    M = np.array(M)\n    v = M.sum(axis=0)\n    if type(v) != np.ndarray:\n        return np.zeros(300)\n    return v / np.sqrt((v ** 2).sum())\n\nxtrain_vector = [sent2vec(x) for x in xtrain[:10]]\nxtrain_vector = np.array(xtrain_vector)\n\n# Option Two: Use the word embeddings directly in deep neural network\n\ninput_layer = Input((max_len, ))\nembedding_layer = Embedding(len(word_index)+1, max_len, weights=[embedding_matrix], trainable=False)(input_layer)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e359bf76ac23848388596149b4c2a729f60600a1"},"cell_type":"markdown","source":"Lets view the word embedding vector"},{"metadata":{"_uuid":"bf963a46c37a5ad9dd361e16daafc917def13e3c","collapsed":true,"_cell_guid":"cec40bc8-6a14-43d5-99b1-70c7cf7a7e8f","trusted":false},"cell_type":"code","source":"# these word vectors can be directly used in the model\nxtrain_vector","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57de1d5ac9d77ebfde5cfc01bf02f69432efc465","_cell_guid":"9d1fb70b-f309-4751-a9b5-dba4ac8cee23"},"cell_type":"markdown","source":"### 7. Topic Modelling Features\n\n**Feature 47:** Topic Modelling Features \n\nI have used LDA for generating Topic Modelling Features. Latent Dirichlet Allocation (LDA) is an algorithm used to discover the hidden topics that are present in a corpus. LDA starts from a fixed number of topics. Each topic is represented as a distribution over words, and each document is then represented as a distribution over topics. Although the tokens themselves are meaningless, the probability distributions over words provided by the topics provide a sense of the different ideas contained in the documents.\n"},{"metadata":{"_uuid":"42c28465fb5d1fcd6a9b1865a62631e354f15d99","collapsed":true,"_cell_guid":"2b948787-491c-48ae-88d2-1fdacbcd0d74","trusted":false},"cell_type":"code","source":"# create count vectorizer first\ncvectorizer = CountVectorizer(min_df=4, max_features=4000, ngram_range=(1,2))\ncvz = cvectorizer.fit_transform(df['text'])\n\n# generate topic models using Latent Dirichlet Allocation\nlda_model = LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20, random_state=42)\nX_topics = lda_model.fit_transform(cvz)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa15431324b2b4f9af1e908e936ad38ccafe0f88","collapsed":true,"_cell_guid":"5bf002cf-9ed0-425d-9d9d-d1e98b5b3525","trusted":false},"cell_type":"code","source":"n_top_words = 10\ntopic_summaries = []\n\n# get topics and topic terms\ntopic_word = lda_model.components_ \nvocab = cvectorizer.get_feature_names()\n\nfor i, topic_dist in enumerate(topic_word):\n    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n    topic_summaries.append(' '.join(topic_words))\n    print('Topic {}: {}'.format(i, ' | '.join(topic_words)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a51f5d37d8034b9decd23f731399418f56b798bf"},"cell_type":"markdown","source":"Lets view some of the topics obtained"},{"metadata":{"_uuid":"73ad04206949199b6bfc3fa12707196de592f834","collapsed":true,"_cell_guid":"28fdae5c-35a9-499f-86e7-a25bf22804f4","trusted":false},"cell_type":"code","source":"X_topics[:5]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea96bf260108a378372b0646f3b3bd0d087b8a63","_cell_guid":"cef2bda4-a2fa-4730-a423-9ec093aed7db"},"cell_type":"markdown","source":"**Please note that the quality of Word Embeddings and Topic Modelling Features \nwill be poor in case of small data run, but they are improved if the models are run on complete data frame**"},{"metadata":{"_uuid":"bffb4a93f08f8fb46cfc903cece09180526ce22b","_cell_guid":"62d4f5ea-fee4-463e-b1ad-2250fce3d5b8"},"cell_type":"markdown","source":"\n### 8. Count Features\n\n**Feature 48 - 60:** Count Features \n\nThere are some categorical features in the dataset, which can be represented as the count features. "},{"metadata":{"_uuid":"cefdd27e8696506a39c1ce638ec66b6f78ec5028","collapsed":true,"_cell_guid":"10c6fa5d-544c-428c-b412-eedd6cfc6a76","trusted":false},"cell_type":"code","source":"features_for_count = ['school_state', 'teacher_id', 'teacher_prefix', 'project_grade_category', 'project_subject_categories', 'project_subject_subcategories']\nfeatures_for_count += ['Year', 'Year_Day', 'Weekday', 'Month_Day', 'Month', 'Hour']\nfor col in features_for_count:\n    aggDF = df.groupby(col).agg('count')\n    aggDF[col] = aggDF.index\n    tempDF = pd.DataFrame(aggDF[['project_submitted_datetime', col]], columns = ['project_submitted_datetime', col])\n    tempDF = tempDF.rename(columns={'project_submitted_datetime': col+\"_count\"})\n    df = df.merge(tempDF, on=col, how='left')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d263caa53741d8593775c7015ec40b1a1f2032c","collapsed":true,"_cell_guid":"fd2429ca-b7ab-4a65-b945-21dee4ea1dae","trusted":false},"cell_type":"code","source":"df[[x+\"_count\" for x in features_for_count]].head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"893ba3712b1c9b5ff2991fc98345f3f1820c9349","collapsed":true,"_cell_guid":"98f704dc-a01f-4021-a7d0-8852b71ef763","trusted":false},"cell_type":"markdown","source":"Thanks for viewing the notebook. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"extensions":{"jupyter_dashboards":{"activeView":"grid_default","version":1,"views":{"report_default":{"name":"report","type":"report"},"grid_default":{"maxColumns":12,"cellMargin":10,"name":"grid","type":"grid","defaultCellHeight":20}}}},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}