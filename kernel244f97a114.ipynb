{"cells":[{"metadata":{"_uuid":"d33de2d6aed662f68a42138c34b568bf718a3a34","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":0,"height":58,"hidden":false,"row":0,"width":12},"report_default":{}}}}},"cell_type":"markdown","source":"<br>\n# <font color=\"red\">!! A Hybrid Approach to Connect Donors to Projects !! </font>\n\n# <font color=\"red\">Using Graphs, Word Embeddings, Context & DeepNets</font>\n\n<br>\n## <font color=\"red\">1. Problem Statement </font>\n\nMajority of the donors on DonorsChoose.org are first time donors. If DonorsChoose.org can motivate even a fraction of those donors to make another donation, that could have a huge impact on the number of classroom requests fulfilled. DonorsChoose.org needs to connect donors with the projects that most inspire them. The idea is to pair up donors to the classroom requests that will most motivate them to make an additional gift.\n\n## <font color=\"red\">1.1 Breakdown of the Problem Statement </font>\n1. The problem statement has two verticals - Donors and Projects  \n2. The target action among the two verticals is to find right connections betwen them.  \n3. The main idea is to find the relevant donors for any given new project.   \n4. Send targeted emails to the recommended donors.    \n\n## <font color=\"red\">1.2 Hypothesis Generation  </font>\n\nHypothesis Generation is the process to undestand the relationships, possible answeres to the problem statement without looking at the data. So before exploring, lets try to answer the key question - What makes a donor donate to a given project? The possible hypothesis could be following - \n\n1. **Altruism**  - Many generous donors might feel that they find it really important to help others. Thus, they make donations.   [Recently](https://www.donorschoose.org/project/have-a-seat-and-lets-move/3270204/), I also made a donation on donorschoose and the feeling was great. \n\n2. **External Influence**  - Some donors might get influenced from others and makes the donation after watching other people donate.  There is also a [study](https://onlinelibrary.wiley.com/doi/full/10.1111/ecoj.12114) which was conducted  on measuring Peer Effects in Charitable Giving. \n\n3. **Social Cause** - Many donors might feel that they give because their donations matter to someone they know and care about. This may be the schools from which they studied, or their teachers from their childhood days.  \n\n4. **Marketing Influence** - Donors might have find it somewhere about the classroom requests, so they tend to donate.  \n\n## <font color=\"red\">1.3 Exploratory Data Analysis </font>\n\nIn my previous kernel on donorschoose, I performed an In-depth exploration in order to understand the dataset which included geography analysis, time series trends, distributions of important variables of the dataset, text based analysis, and comparisons.   \n\nLink : https://www.kaggle.com/shivamb/a-visual-and-insightful-journey-donorschoose\n\n\n![](https://i.imgur.com/ml7uzlr.png)\n\n"},{"metadata":{"_uuid":"e9386045e69d7456e678e899700ab0e90facd9ae","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":0,"height":65,"hidden":false,"row":58,"width":12},"report_default":{}}}}},"cell_type":"markdown","source":"# <font color=\"Red\">2. Approach - Connecting Donors to Classroom Requests</font>\n\n\nBased on the problem statement, insights from eda, business intution, hypothesis generation, and thought process, the overview of my approach is shown in the following figure and is described afterwards:  \n\n<br>\n\n![DonorsChoose%20%281%29.png](https://i.imgur.com/htGl6pf.jpg)\n\n<br>\n\nThe main ideas behind the appraoch are:\n\n1. Connect donors whose features are relevant to the given project  (project - donor similarity)  \n2. Connect donors of the projects whose features are contextually similar to the given project (project - project similarity)   \n3. Find similar donors using donor - donor similarity   (donor - donor similarity)    \n4. Cluster the donors on the basis of their contextual details (context aware recommendations)  \n5. Find the donors who are likely to make donation in a given project on the basis of their past behaviours (Donors who donated in these projects also donated in these other projects )\n\nIn the implementation part, I am making use of following: \n\n1. A Graph based approach to represent the donor profiles, project profiles, connections among the donors, connection among the projects. \n2. Training a deep neural network using the past donor project interactions to generate the donor-project relevance score. \n3. Ensemble of the two parts are given as the final results      \n\n**Part A: Graph Based Recommendations**\n    \nIn this approach, the information of the donors and the projects is saved individually in separate graph like structures. Every node of these graphs repersents either a donor or a project, the edges between them represents how similar the two nodes are. The process flow is defined below:  \n    \n>    1. First, the projects and donors are profiled individually based on their past data. In this step, additional features for the donors and projects are also created using aggregations, feature engineering techniques, and word embedding vectors.   \n>    2. There are two types of features in the dataset - text features and non-text features. From my learnings from the [previous](https://www.kaggle.com/c/donorschoose-application-screening) donorschoose competition and insights from eda, I realize that text features plays an imporant role in describing the context of the projects. Using the pre-trained models, the word embeddings vectors of text features are then created.   \n>    3. The complete information is then represented in the form of graphs in which every nodes repersents either a donor or a project.  \n>    4. To create the edges among the nodes graphs, similarity algorithms such as cosine similairty, jaccard similarirty, and pearson correlation are run. The edges are then created among the donor - donor nodes, project - project nodes.   \n>    5. To find the relevant donors for a given project, its edges are created with the donors graph using the same similarity algorithm. The most strong connections represents first set of suggested donors for a project.  \n>    6. This set is further extended by finding the donors of the similar projects using projects graph.  Also, other similar donors are added as the recommended set.  \n>    7. Using the non text features, the final recommendations are filtered. Those features act as the filters to the final set.  \n\n**Context Aware**  \n>   One of the important vertical of the donorschoose data is the donors. It is worth the effort to enrich the additional details or context about the donors. Using external datasets, donor profiles can be enriched with more features which can be used in the overall recommendation model. This idea is inspired from the main intution behind **context aware recommendations**.     The more details about this section are given in section 5.1  \n    \n \n**Part B: Recommendations using DeepNets**\n\nThe key idea behind this part is evaluating the past behaviours of donors to measure if a given donor - project combination is significant or not. All the past project donor interactions are represented in the form of a matrix in which every row repersent a project, every column a donor and every cell value represnt the interaction score. In this model, Donation amount is choosen as the proxy for the interaction score.  The process flow is defined below:  \n\n> 1. In this approach, the past interactions of donors and projects are represnted in the form of embedding layers.    \n> 2. The embedding layers for donors and the projects are concatenated together. The resultant vector is then passed from multiple multi-layer perceptrons layers.  \n> 3. In the model training process, the embedding parameters are learned which gives a latent representation of every project donor interaction. The learned model then tries to predict the interaction score for a given donor - project combination.   \n> 4. Final recommendations of the donors for a project are generated by predicting the interaction scores for every combination.  Donors having higher score are recommended.  \n\n**Final Results : finding the donors of a given project ** \n\nThe final results are given as the combination of results obtained in part A and part B.  \n\nIn this whole workflow, some initial filters are applied on the dataset such as considering only the projects which are Fully Funded and the donors which are not teachers. \n\n<br><hr>\n# <font color=\"red\"> 3. Dataset Preparation   </font>\n\nTo begin with, Import all the rquired python modules to be used in the entire framework.  "},{"metadata":{"_uuid":"d052df9a2a2fde1aa937eb91345a5218b9f4216c","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"# keras modules \nfrom keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\nfrom keras.layers import Embedding, Reshape, Dropout, Dense, Input, Concatenate\nfrom keras.models import Sequential, Model\n\n# sklearn modules \nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.cluster import KMeans \nfrom sklearn import preprocessing \nimport matplotlib.pyplot as plt\n\n# other python utilities \nfrom collections import Counter \nfrom tqdm import tqdm \nimport pandas as pd \nimport numpy as np \nimport warnings, math\nimport gc \n\n# ignore the warnings \nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb0aa88348628f836f335c763711dfab504fba43","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":0,"height":4,"hidden":false,"row":123,"width":12},"report_default":{}}}}},"cell_type":"markdown","source":"## <font color=\"red\">3.1 Load Dataset</font> \nLoad the dataset files into pandas dataframe. I am using four datasets - donors, donations, projects, schools."},{"metadata":{"_uuid":"7e3d00d1087f844c681bd8a235a40dfb27455b0c","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"path = \"../input/io/\"\ndonors_df = pd.read_csv(path+\"Donors.csv\")\ndonations_df = pd.read_csv(path+\"Donations.csv\")\nschools_df = pd.read_csv(path+\"Schools.csv\")\nprojects_df = pd.read_csv(path+\"Projects.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d711df080f69d249372380e841d1a08cd190eb13","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":0,"height":4,"hidden":false,"row":127,"width":12},"report_default":{}}}}},"cell_type":"markdown","source":"## <font color=\"red\">3.2 Merge Datasets</font>\n\nMerge the input datasets together so that all the features can be used for the project and user profiling. Also, create additional features in the projects data related to Posted Year and Posted Month.  "},{"metadata":{"_kg_hide-output":true,"_uuid":"b6b17f374f3079f703acaf78945b1ffd12f078a5","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":0,"height":4,"hidden":false,"row":131,"width":4},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"## Merge donations and donors \ndonation_donor = donations_df.merge(donors_df, on='Donor ID', how='inner')\n\n## Merge projects and schools \nprojects_df = projects_df.merge(schools_df, on='School ID', how='inner')\n\n## Create some additional features in projects data\nprojects_df['cost'] = projects_df['Project Cost']\\\n                      .apply(lambda x : float(str(x).replace(\"$\",\"\").replace(\",\",\"\")))\nprojects_df['Posted Date'] = pd.to_datetime(projects_df['Project Posted Date'])\nprojects_df['Posted Year'] = projects_df['Posted Date'].dt.year\nprojects_df['Posted Month'] = projects_df['Posted Date'].dt.month\n\n## Merge projects and donations (and donors)\nmaster_df = projects_df.merge(donation_donor, on='Project ID', how='inner')\n\n## Delete unusued datasets and clear the memory\ndel donation_donor, schools_df\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6e41e33500dfd9484cb8a22929d40e3255c5d33","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":0,"height":23,"hidden":false,"row":135,"width":12},"report_default":{}}}}},"cell_type":"markdown","source":"# <font color=\"red\">4. Projects Profiling</font> \n\nProfiling of projects is a pretty straightforward task because every row in the projects dataset repersents the features of a particular project. Every row in this data is itself a profile of a particular project. For example - \n\n    1. Project A : \n        - Project Title : Need Flexible Seating  \n        - Project Category : Supplies    \n        - Project State : Texas  \n        - Posted Year : 2017  \n        - Project Cost : 1000\n        - ...  \n        \n        \n    2. Project B : \n        - Project Title : Need Chromebooks  \n        - Project Category : Technology  \n        - Project State : Florida  \n        - Posted Year : 2016  \n        - Project Cost : 500\n        - ...\n\nEvery feature repersents some information about the project. One of the important feature about projects are the text features such as project essay. To use project essays as feature of the project, I will use pre-trained word embedding vectors from fastText.  \n\n## <font color=\"red\">4.1 Create Word Embedding Vectors</font>  \n\nWord embeddings are the form of representing words and documents using a dense vector representation. The position of a word within the vector space is learned from text and is based on the words that surround the word when it is used. Word embeddings can be trained using the input corpus itself or can be generated using pre-trained word embeddings such as Glove, FastText, and Word2Vec. Any one of them can be used as transfer learning. I am using fastText English Word Vectors from Common Crawl as the pre-trained vectors. These vectors contain 2 million word vectors trained on 600B tokens. \n\nSource of the fastText vectors - https://www.kaggle.com/facebook/fatsttext-common-crawl/data  \n"},{"metadata":{"_kg_hide-output":true,"_uuid":"11564b1101faf0d16a331500679b4ff66e53362c","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"## Create a smaller version of data so that it runs on kaggle kernel\n## keep only fully funded projects\nprojects_mini = projects_df[projects_df['Project Current Status'] == \"Fully Funded\"]\n\n## Set rows = -1 to run on complete dataset, To run in kaggle kernel, I am setting to a smaller number \nrows = 5000\n\n## keep only the projects of 2017, quarter 3, take small sample, (so that it runs on kaggle kernels)\nif rows != -1:\n    projects_mini = projects_mini[(projects_mini['Posted Year'] == 2017) &\n                                  (projects_mini['Posted Month'] > 9)]\n    projects_mini = projects_mini.reset_index()[:rows]\n\n## replace the missing values and obtain project essay values \nprojects_mini['Project Essay'] = projects_mini['Project Essay'].fillna(\" \")\nxtrain = projects_mini['Project Essay'].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca32095f74ca522c711d300a8e197e2a98dbec70","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":4,"height":4,"hidden":false,"row":131,"width":4},"report_default":{}}}}},"cell_type":"markdown","source":"First of all load the pre-trained word vectors from fastText Common Crawl into the python dictionary.  This dictionary will be used to get the word embedding vector of a given word in the project essay. "},{"metadata":{"_uuid":"79c6adbd2f865ad561e6c4314bf1ee4f0d2ebd19","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"EMBEDDING_FILE = '../input/fatsttext-common-crawl/crawl-300d-2M/crawl-300d-2M.vec'\n\nembeddings_index = {}\nf = open(EMBEDDING_FILE, encoding=\"utf8\")\ncount = 0\nfor line in tqdm(f):\n    count += 1\n    ## Remove this if condition to read 2M rows \n    if count == 500000: \n        break\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2fff6611b2cde78bc86d992a94b2399afcbccda","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":0,"height":4,"hidden":false,"row":158,"width":12},"report_default":{}}}}},"cell_type":"markdown","source":"Now the word embedding vectors are loaded, lets convert the project essay values to document vectors. For this purpose, I have created a function which can be used to compute the average word embedding vector of an entire document."},{"metadata":{"_uuid":"f7bf425b2e3d07ac59662ba54dc011546813bd0b","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"def generate_doc_vectors(s):\n    words = str(s).lower().split() \n    words = [w for w in words if w.isalpha()]\n    M = []\n    for w in words:\n        if w in embeddings_index:\n            M.append(embeddings_index[w])\n    v = np.array(M).sum(axis=0)\n    if type(v) != np.ndarray:\n        return np.zeros(300)\n    return v / np.sqrt((v ** 2).sum())\n\nxtrain_embeddings = [generate_doc_vectors(x) for x in tqdm(xtrain)]\n\ndel xtrain\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a26e3e8a607b555d910a97f74964b1f0975aca64","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":8,"height":4,"hidden":false,"row":131,"width":4},"report_default":{}}}}},"cell_type":"markdown","source":"Before using the word embeddings, lets normalize them using Standard Scaler"},{"metadata":{"_uuid":"c2d1c72af93760a5ab750523afa5ffbe18ac22a7","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"scl = preprocessing.StandardScaler()\nxtrain_embeddings = np.array(xtrain_embeddings)\nxtrain_embeddings = scl.fit_transform(xtrain_embeddings)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6091b9cf084f3ba7c44cd97183af8c838a45b23","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":0,"height":4,"hidden":false,"row":162,"width":4},"report_default":{}}}}},"cell_type":"markdown","source":"Lets take a look at the project profiles and some of their features "},{"metadata":{"_uuid":"e2597143dc285b974347966e6770fbd2938c62be","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"projects_mini[['Project ID', 'Project Title', 'Project Subject Category Tree', 'Project Resource Category', 'Project Cost']].head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e4341438b44829a22a81f1ad8f11cc10babe3c2","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":4,"height":4,"hidden":false,"row":162,"width":4},"report_default":{}}}}},"cell_type":"markdown","source":"Lets take a look at the word embedding vectors for every project"},{"metadata":{"_uuid":"43d38390b0af0025da3a076f5211dd715b9ed1d9","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"print (\"Project ID: \" + projects_mini['Project ID'].iloc(0)[0])\nprint (\"Project Vector: \")\nprint (xtrain_embeddings)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f4986fe3b75950f6ab3adcf2dda3752fb2cb44d","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":0,"height":37,"hidden":false,"row":166,"width":12},"report_default":{}}}}},"cell_type":"markdown","source":"# <font color=\"red\">5. Donors Profiling </font>\n\nNow, lets profile the donors using their past donations. The aggregated results from the past donations made by the donors will indicate the characteristics of the donors. However, there exists a cold start with the donors who have made only single donation in the past. Basically which means that their profiles will not depict a true picture about their characteristics. \n\nCold Start Problem : Donors with Single Donation\n\nSo as we start aggregating the profiles, we will end be creating more and more better profiles if the number of donations by a particular donor is higher while it will be poor if the donations are lesser. \n\n<table align=\"center\">\n    <tr><td>Donor</td><td> Number of Projects Donated</td><td>Profiling</td></tr>\n    <tr><td>A</td><td>1</td><td>Less Accurate Profiling</td></tr>\n    <tr><td>B</td><td>2 - 5</td><td>Average Profiling</td></tr>\n    <tr><td>C</td><td>5 - 10</td><td>Good Profiling</td></tr>\n    <tr><td>D</td><td>10 - 100</td><td>Very Good Profiling</td></tr>\n    <tr><td>E</td><td>100+</td><td>Excellent Profiling</td></tr>\n</table>\n\n\n\n\nIn my approach I am handelling this part using two ways, one is to use external datasets to add more context in the profiles and second is to simply ignore the fact that number of donations are less and use the features as it is. The intution about second approach is that atleast there will be initial level understanding about the donors. \n\n1. Enrichment using external context \n2. Use the first project of the donor  \n\n## <font color=\"red\">5.1 Context Enrichment</font>\n\nThe first part is somewhat close to **context aware recommendation** engines in which the model uses additional context to measure the similarity of a new donor with the other donors in order to create the donor profiles. \n\nIn this technique, donor profiles can be enriched with some external data which will be used as features to find similar donors. In the given donorschoose dataset, very few details about the context / persona of donors is given. Example of context are shown in the following image. \n\nSo basically in this section, we will create additional features which can be added to the donors profile. \n\n<br><br>\n\n![context-aware.png](https://i.imgur.com/dMU0his.jpg)\n\n\n<br><br>\n\nContext can be defined in any terms such as the social media information of the donor, the credit card or the banking transactions of the donor, the demographics details of the donor, or the details about the area from where they belong. \n\nThe main idea of using context is to understand better about what context of donors are likely to donate. This data can help to establish some examples such as :  \n\n1. Donors which belong to metro cities having high unemployment rate are probably similar.  \n2. Donors which belong to areas where population is higher tends to donate for health related projects  \n3. Donors which belong to areas where median income is higher they tend to donate in rural areas  \n4. Donors which belong to cities where median home value is lower, they tend to donate for seating projects   \n\n\nTo implement this part I made use of the external data about donor areas and enriched details such as populatiom, population density, median home value, household income etc about their areas. \n\nSource of the dataset : https://www.unitedstateszipcodes.org/\n\nIn this kernel, I am using only a smaller data about the donor areas. But I have posted a [gist](https://gist.github.com/shivam5992/7d346da49930b1f4726f0700366a1dd2) in order to get data for the entire donorschoose donors data. "},{"metadata":{"_kg_hide-output":true,"_uuid":"fd701aac002713d07aab3daf283715e50e7310a2","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"# slicing the users dataset so that it runs in kaggle kernel memory \nusers = master_df[(master_df['Donor Is Teacher'] == 'No') & \n                  (master_df['Donor State'] == 'Texas') & \n                  (master_df['Posted Year'] == 2017)].reset_index()\n\nusers1 = users[:1000]\ndel master_df\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"071ac32e702b6fac884dde0b821db04ae2257276"},"cell_type":"markdown","source":"#### <font color=\"red\"> Load external data </font>"},{"metadata":{"_uuid":"75ca7f9f9d0c5aa2f2809ad91f83d77b8c5b5161","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"## load the external dataset\nexternal_context = pd.read_csv('../input/external-context/area_context_texas.csv')\nfeatures = list(set(external_context.columns) - set(['id', 'zipcode']))\nagg_doc = {}\nfor feat in features:\n    agg_doc[feat] = 'mean'\n\narea_context = external_context.groupby('id').agg(agg_doc).reset_index().rename(columns = {'id' : 'Donor Zip'})\narea_context = area_context[area_context['Housing Units'] != 0]\narea_context.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f77f3c5883740b8c7ef8954f014ac2f3fdbaa5f"},"cell_type":"markdown","source":"#### <font color=\"red\">Clustering the external data </font>\n\nThe idea here is that we need to create additional feature which reflects the context of the donor area (or something else). One technique which can help here is the clustering of external data and use the clusters as the features. Lets do that using K-means clustering.  \n\nFind the right K for K-means clustering of the data using Elbow method. "},{"metadata":{"_uuid":"cf132ccb7b33a47346737bcba4204d74a5f879d4","trusted":true,"collapsed":true},"cell_type":"code","source":"features = list(set(area_context.columns) - set(['Donor Zip']))\ninretia = []\nfor i in range(1,11):\n    kmeans = KMeans(n_clusters=i,init='k-means++',max_iter=300,n_init=10,random_state=0)\n    kmeans.fit(area_context[features])\n    inretia.append(kmeans.inertia_)\nplt.plot(range(1,11),inretia)\nplt.title('Finding the Optimal Number of Clusters')\nplt.xlabel('Number of clusters')\nplt.xlabel('Kmeans Inretia')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96c4f00062c88b18661b24494d4c594afd126f5a"},"cell_type":"markdown","source":"From the graph, we can see that this data can be clustered better with K = 3"},{"metadata":{"_uuid":"deb6edc50a3978bb4681318c05adb63d802250a4","trusted":true,"collapsed":true},"cell_type":"code","source":"# apply kmeans clustering \nkmeans = KMeans(n_clusters=3, init='k-means++', max_iter=300, n_init=10, random_state=0) \narea_context['area_context_cluster'] = kmeans.fit_predict(area_context[features])\n\n# merge with the donors data\nusers1['Donor Zip'] = users1['Donor Zip'].astype(str)\narea_context['Donor Zip'] = area_context['Donor Zip'].astype(str)\n\nusers1 = users1.merge(area_context[['Donor Zip', 'area_context_cluster']], on=\"Donor Zip\", how=\"left\")\narea_context[['Donor Zip', 'area_context_cluster']].head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68e1dfa843f3e2997bdaf29e2b6c3fff0faba76f","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":0,"height":10,"hidden":false,"row":203,"width":12},"report_default":{}}}}},"cell_type":"markdown","source":"So basically we have now generated additional feature which can be added to the donors profiles and can be used to find similar users, as we will see in the next sections. In the similar manner, with more additional data, more context clusters can be created and added to the donors profile. \n\n- The second approach:\n\nWhile it is true that donors having single project donation will not have enough data to create a good profile, but atleast they have some data which can be used to understand the preferences / likes / behaviour of the donor. So in thie approach, I am not removing the donors having single donation. \n\n## <font color=\"red\"> 5.2 Feature Engineering - Donor Profiles</font>\n\nLets create the word embedding vectors for all the projects in which donors have donated. Later on we will use these vector and aggregate them for obtaining average project vector for donor. "},{"metadata":{"_uuid":"bfa0417eb345ed085e63d154408900f1ef30acaf","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"users1['Project Essay'] = users1['Project Essay'].fillna(\" \")\nutrain = users1['Project Essay'].values\n\nutrain_embeddings = [generate_doc_vectors(x) for x in tqdm(utrain)]\nutrain_embeddings = np.array(utrain_embeddings)\nutrain_embeddings = scl.fit_transform(utrain_embeddings)\n\ndel utrain\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e81ec25b035eb6a9a69c406407c252f9c4c925dd","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":0,"height":8,"hidden":false,"row":213,"width":12},"report_default":{}}}}},"cell_type":"markdown","source":"## <font color=\"red\">5.3 Aggregate Donors Data - Create Donors Profiles </font>\n\nLets aggregate the users data and obtain the features related to donors based on their past donations. I am using the following features: \n\n1. Donation Amount : Min, Max, Median, Mean  \n2. Project Cost : Min, Max, Median, Mean  \n3. Number of Projects Donated  \n4. Project Subject Category Distribution  \n5. Project Grade Level Distribution  \n6. Project Essays - Average Word Embedding Vector  "},{"metadata":{"_uuid":"c77de414551d0b4f83bee840775694a043c0fc87","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"## handle few missing values \nusers1['Project Type'] = users1['Project Type'].fillna(\"Teacher Led\")\nusers1['Project Subject Category Tree'] = users1['Project Subject Category Tree'].fillna(\" \")\nusers1['area_context_cluster'] = users1['area_context_cluster'].astype(str)\n\n## aggregate the donors and their past donations in order to create their donor - profiles\nuser_profile = users1.groupby('Donor ID').agg({'Donation Amount' : ['min', 'max', 'mean', 'median'],\n                                               'cost' : ['min', 'max', 'mean', 'median'], \n                                      'Project Subject Category Tree' : lambda x: \", \".join(x), \n                                      'Project ID' : lambda x: \",\".join(x), \n                                      'School Metro Type' : lambda x: \",\".join(x), \n                                      'Project Title' : lambda x: \",\".join(x), \n                                      'area_context_cluster' : lambda x: \",\".join(x), \n                                      'School Percentage Free Lunch' : 'mean',\n                                      'Project Grade Level Category' : lambda x : \",\".join(x),\n                                      'Project Type' : 'count'}\n                                    ).reset_index().rename(columns={'Project Type' : \"Projects Funded\"})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71753df9d6aba8e579e3d10018fa0ee14c004147","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"## flatten the features of every donor\n\ndef get_map_features(long_text):\n    a = long_text.split(\",\")\n    a = [_.strip() for _ in a]\n    mapp = dict(Counter(a))\n    return mapp\n    \nuser_profile['Category_Map'] = user_profile['Project Subject Category Tree']['<lambda>'].apply(get_map_features)\nuser_profile['Projects_Funded'] = user_profile['Project ID']['<lambda>'].apply(get_map_features)\nuser_profile['GradeLevel_Map'] = user_profile['Project Grade Level Category']['<lambda>'].apply(get_map_features)\nuser_profile['AreaContext_Map'] = user_profile['area_context_cluster']['<lambda>'].apply(get_map_features)\nuser_profile['SchoolMetroType_Map'] = user_profile['School Metro Type']['<lambda>'].apply(get_map_features)\nuser_profile = user_profile.drop(['Project Grade Level Category', 'Project Subject Category Tree',  'School Metro Type', 'Project ID', 'area_context_cluster'], axis=1)\n\nuser_profile.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b69982ea40ccf4e8c29b20fe0dd666b01600a79","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":0,"height":4,"hidden":false,"row":221,"width":4},"report_default":{}}}}},"cell_type":"markdown","source":"## <font color=\"red\">5.4 Create Donor Feature Vector </font>\n\nTake the average of project vectors in which users have donated "},{"metadata":{"_uuid":"ed3babe8944e3cf79bb1485de061e426443393e7","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"def get_average_vector(project_ids):\n    ids = list(project_ids.keys())\n    \n    donor_proj_vec = []\n    for idd in ids:        \n        unique_proj_ids = users1[users1['Project ID'] == idd].index.tolist()[0]\n        donor_proj_vec.append(utrain_embeddings[unique_proj_ids])\n    proj_vec = np.array(donor_proj_vec).mean(axis=0)\n    return proj_vec \n\nuser_profile['project_vectors'] = user_profile['Projects_Funded'].apply(lambda x : get_average_vector(x))\nuser_profile[['Donor ID', 'project_vectors']].head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e27a0f9d726f9203283e7dfc168a23ec5f4e5f5","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":0,"height":15,"hidden":false,"row":225,"width":12},"report_default":{}}}}},"cell_type":"markdown","source":"So, now we have created the nodes for the donors graph and the projects graph. Every node repersents the information about a particular donor / project. In the next section, lets create the interaction matrices which will help to create the edges between the nodes in the donors / projects graphs. \n\n# <font color=\"red\">6. Creating the Interaction Matrices</font>\n\nInteraction matrices repersents the relationships between the any two nodes of the graphs. For example, consider a n * n project - project matrix in which every row repersents a project, every column repersent other project and the value at cell i, j repersent the interaction score of the two projects.  \n\nOne way to measure the interaction score is to compute the similarity among the two nodes. For example, if two projects has higher number of similar and correlated features, the similarity score for them will be higher, and so is the interaction score. A number of different techniques can be used to measure the similarity among the two nodes / vectors such as Jaccard Similarity, Ngram Similarity, Cosine Similarity etc. In this kernel I am using cosine similarity. \n\n## <font color=\"red\">6.1 Project - Project Edges </font>\n\n### <font color=\"red\">6.1.1 Create Project Interactions </font>  \n\nLets first compute the project interaction scores using cosine similarities. Save the results in a variable. \n\n**Note -** At this point, while computing the similarity, I am currently using text features to measure the similarities. Other non text features will be used later to filter the recommendations and predictions in the next sections."},{"metadata":{"_uuid":"0f170444fa11fa9e7761b04be37baf0170103ae2","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"# compute the project project interactions \nproject_interactions = linear_kernel(xtrain_embeddings, xtrain_embeddings)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00ec9f92afbe7f0b743722204bdd494251964473","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":4,"height":4,"hidden":false,"row":221,"width":4},"report_default":{}}}}},"cell_type":"markdown","source":"### <font color=\"red\">6.1.2 Find top similar nodes  </font>  \n\nNext, iterate for every node (project) and compute its top similar nodes"},{"metadata":{"_uuid":"5b52da462bc9d2eca79e517e2bc2c90acc06e3d9","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"# create the edges of one node with other most similar nodes  \nproject_edges = {}\nfor idx, row in projects_mini.iterrows():\n    similar_indices = project_interactions[idx].argsort()[:-100:-1]\n    similar_items = [(project_interactions[idx][i], projects_mini['Project ID'][i]) for i in similar_indices]\n    project_edges[row['Project ID']] = similar_items[:20]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b852c2bb5f48290624b877395373a9e95b598a2e","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":0,"height":4,"hidden":false,"row":240,"width":12},"report_default":{}}}}},"cell_type":"markdown","source":"### <font color=\"red\">6.1.3 Write a function to obtain similar nodes </font>  \n\nLets write a function which can be used to obtain the most similar project, similarity or the interaction score along with the project features. "},{"metadata":{"_uuid":"2687145aaa1e542d51e9384a8158cd2cc32a709b","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"def get_project(id):\n    return projects_mini.loc[projects_mini['Project ID'] == id]['Project Title'].tolist()[0]\n\ndef similar_projects(project_id, num):\n    print(\"Project: \" + get_project(project_id))\n    print(\"\")\n    print(\"Similar Projects: \")\n    print(\"\")\n    recs = project_edges[project_id][1:num]\n    for rec in recs:\n        print(get_project(rec[1]) + \" (score:\" + str(rec[0]) + \")\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e5f764792b0a67462f54b51b16b1d7ada5eef74","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":8,"height":4,"hidden":false,"row":221,"width":4},"report_default":{}}}}},"cell_type":"markdown","source":"Lets obtain some projects and their similar projects "},{"metadata":{"_uuid":"84b93c90550f03f776a0a10a47a37e9aa4f69817","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"similar_projects(project_id=\"a0446d393feaadbeb32cd5c3b2b36d45\", num=10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b52cf24fb1d37085779dfdbaf257c5cc27f14457","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"similar_projects(project_id=\"83b4f3fbe743cb12ae2be7347ef03ecb\", num=10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ce07c13eda4fe3f92ca79d6a0b160aac02e6db4","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":0,"height":5,"hidden":false,"row":244,"width":12},"report_default":{}}}}},"cell_type":"markdown","source":"## <font color=\"red\">6.2 Donor - Donor Edges </font>  \n\n### <font color=\"red\">6.2.1 Create Donor Interactions </font>  \n\nSimilar to project project interactions, we can compute the donor - donor interactions using their profiles and the context."},{"metadata":{"_uuid":"c06d00da436ac59e813792aa40397914b65ccf94","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"user_embeddings = user_profile['project_vectors'].values\n\nuser_embeddings_matrix = np.zeros(shape=(user_embeddings.shape[0], 300))\nfor i,embedding in enumerate(user_embeddings):\n    user_embeddings_matrix[i] = embedding\n\ndonors_interactions = linear_kernel(user_embeddings_matrix, user_embeddings_matrix)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72f3ff6d758bf52a27abaf657d9eb06019f6c132","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":0,"height":4,"hidden":false,"row":249,"width":4},"report_default":{}}}}},"cell_type":"markdown","source":"### <font color=\"red\">6.2.2 Find top similar nodes</font>  \n\nNext, iterate for every node (donor) and compute its top similar nodes"},{"metadata":{"_uuid":"4b02edde9bb07894f9f04f6e5bd866c14be8fe0c","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"user_edges = {}\nfor idx, row in user_profile.iterrows():\n    similar_indices = donors_interactions[idx].argsort()[:-10:-1]\n\n    similar_items = [(float(donors_interactions[idx][i]), list(user_profile['Donor ID'])[i]) for i in similar_indices]\n    user_edges[row['Donor ID'][0]] = similar_items[1:]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"836bb1be2ecfd9a36c104e5bd1e1d68848c22f2c","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":0,"height":4,"hidden":false,"row":253,"width":12},"report_default":{}}}}},"cell_type":"markdown","source":"### <font color=\"red\">6.2.3 Write a function to obtain the similar nodes </font>  \n\nLets write a function which can be used to obtain the most similar donors, similarity or the interaction score along with the donor features. "},{"metadata":{"_uuid":"c65446a796a43a5ef498582773895faa19a86b6d","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"def get_donor(id):\n    return user_profile.loc[user_profile['Donor ID'] == id]['Donor ID'].tolist()[0]\n\ndef similar_users(donor_id, num):\n    print(\"Donor: \" + get_donor(donor_id))\n    print (\"Projects: \" + str(user_profile[user_profile['Donor ID'] == donor_id]['Project Title']['<lambda>'].iloc(0)[0]))\n\n    print(\"\")\n    print(\"Similar Donors: \")\n    print(\"\")    \n    recs = user_edges[donor_id][:num]\n    for rec in recs:\n        print(\"DonorID: \" + get_donor(rec[1]) +\" | Score: \"+ str(rec[0]) )\n        print (\"Projects: \" + str(user_profile[user_profile['Donor ID'] == rec[1]]['Project Title']['<lambda>'].iloc(0)[0]))\n        print   (\"\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3adb5bbf3d23533905ebbbb87e3060c70dff42ad","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":4,"height":4,"hidden":false,"row":249,"width":4},"report_default":{}}}}},"cell_type":"markdown","source":"Lets view some donors and their similar donors"},{"metadata":{"trusted":true,"_uuid":"ad29fca6f47fa595ca28dd33d374a0b9764af370","collapsed":true},"cell_type":"code","source":"similar_users(donor_id=\"fee882faa77bc6691bd24d4d5abd5733\", num=5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d5b6f1d83b1289d1666995d7b4c5d53816cb8fa","trusted":true,"collapsed":true},"cell_type":"code","source":"similar_users(donor_id=\"d52242e9d5006fb97fcdb5565982f0ad\", num=5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"757118507bd2a1c9c59a2d1dfae4656f55820151","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":0,"height":17,"hidden":false,"row":257,"width":12},"report_default":{}}}}},"cell_type":"markdown","source":"So we can see that model is picking the results having similar content features. \n\nDonors having similar features as of the given project are recommended. Example - donors from the projects **\"Books Forever\", \"For the Love of Reading\",  \"We like Buddy Books and We Cannot Lie\"** are suggested as one part of the set, This is because the features of the donors who donated in these projects have matched with the features of the given project.  \n\nIn case of Project titled **Learning through Art**, similar projects also had features related to arts / learning etc. for example - **Printmaking Production**, **Art & Crafts Materials for PreK** etc. For project related to food and health category, other similar projects were also picked from the same categories such as breakfast options, food options, heatlhy styles etc. \n\n## <font color=\"red\">7. Building the Graphs </font>\n\nIn the previous sections, we have created the nodes for donors and projects, interaction matrices for donors and projects. Now we can create the graph like structures to store this information. One plus point of using graphs is that it can help to find the connections really quick. \n\n## <font color=\"red\">7.1 Donors Graph </font>\n\nWrite a class for creating Donors Graph. The main functions that I will use are :\n\n    _create_node : function to add a new node in the graph  \n\n    _create_edges : function to create the edges among the nodes  \n\n    _view_graph : function to view the graph  "},{"metadata":{"_uuid":"a31ee950cd94e3c991b283a1ae1888684aa1efe3","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"class DonorsGraph():\n    \"\"\"\n    Class to create the graph for donors and save their information in different nodes.\n    \"\"\"\n    \n    \n    def __init__(self, graph_name):\n        self.graph = {}\n        self.graph_name = graph_name\n    \n    # function to add new nodes in the graph\n    def _create_node(self, node_id, node_properties):\n        self.graph[node_id] = node_properties \n    \n    # function to view the nodes in the graph\n    def _view_nodes(self):\n        return self.graph\n    \n    # function to create edges\n    def _create_edges(self, node_id, node_edges):\n        if node_id in self.graph:\n            self.graph[node_id]['edges'] = node_edges","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"892936306e9af7b885ae7bf76b1db8e1bc6a965b","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":8,"height":4,"hidden":false,"row":249,"width":4},"report_default":{}}}}},"cell_type":"markdown","source":"### <font color=\"red\">7.1.1 Add the Donors Nodes </font>  \n\nInitialize the donors graph and iterate in donor proflies to add the nodes with their properties"},{"metadata":{"_uuid":"4deacb15576a3988919fc115afab4f9cec950404","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"## initialize the donors graph\ndg = DonorsGraph(graph_name = 'donor')\n\n## iterate in donor profiles and add the nodes\nfor idx, row in user_profile.iterrows():\n    node_id = row['Donor ID'].tolist()[0]\n    node_properties = dict(row)\n    dg._create_node(node_id, node_properties)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c6bb13b88050f04d08c41bfd2a326efd83abc3e","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":0,"height":4,"hidden":false,"row":274,"width":4},"report_default":{}}}}},"cell_type":"markdown","source":"Lets view how the features of a node looks like.\n\nAggregated project vectors:"},{"metadata":{"_uuid":"011b4a38c43789d5b844e73ceda48cafd4126aa1","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"node = dg._view_nodes()['12d74c3cd5f21ed4b17c781da828d076']\nnode[('project_vectors','')][0:50]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55712a6a80015c068858d95b0d628201d2b04f43","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":4,"height":4,"hidden":false,"row":274,"width":4},"report_default":{}}}}},"cell_type":"markdown","source":"Aggregated other features"},{"metadata":{"_uuid":"c0de85f1271936db3b62e98b263ce3fe67b271bd","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"del node[('project_vectors','')]\nnode","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"621af8fd7303410a707023dd18cf9710bcab05c3","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":0,"height":4,"hidden":false,"row":278,"width":12},"report_default":{}}}}},"cell_type":"markdown","source":"### <font color=\"red\">7.1.2 Add the Edges among Donors Nodes </font>  \n\nUsing interaction matrices created earlier, create the edges among donor nodes "},{"metadata":{"_uuid":"f9be6d38c0d9223eda73020b4a4841ed735c9f80","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"def get_donor(id):\n    return user_profile.loc[user_profile['Donor ID'] == id]['Donor ID'].tolist()[0]\n\ndef get_similar_donors(donor_id, num):\n    # improve this algorithm - > currently only text, add other features as well \n    recs = user_edges[donor_id][:num]    \n    return recs \n\nfor idx, row in user_profile.iterrows():\n    node_id = row['Donor ID'].tolist()[0]\n    node_edges = get_similar_donors(donor_id=node_id, num=5)\n    dg._create_edges(node_id, node_edges)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c349075f79d02f41f4af1211f0c9dadd4ee98e4d","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":8,"height":4,"hidden":false,"row":274,"width":4},"report_default":{}}}}},"cell_type":"markdown","source":"Lets view the updated nodes"},{"metadata":{"_uuid":"941c2c62b758d376899fdb0d015d8911d4c3b999","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"dg._view_nodes()['00b3c149822c79e4fca9be0bea5c900c']['edges']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"54425ecd3bb77b435e34fc836aa8a1f6028db94e","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":0,"height":8,"hidden":false,"row":282,"width":12},"report_default":{}}}}},"cell_type":"markdown","source":"## <font color=\"red\">7.2 Projects Graph </font>\n\nWrite a class for creating Projects Graph. The main functions are similar to donors graph :\n\n    _create_node : function to add a new node in the graph  \n\n    _create_edges : function to create the edges among the nodes  \n\n    _view_graph : function to view the graph  "},{"metadata":{"_uuid":"682f89ae0af9f6b728d83ed322be10f9eab2c336","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"class ProjectsGraph():\n    def __init__(self, graph_name):\n        self.graph = {}\n        self.graph_name = graph_name\n        \n    def _create_node(self, node_id, node_properties):\n        self.graph[node_id] = node_properties \n    \n    def _view_nodes(self):\n        return self.graph\n    \n    def _create_edges(self, node_id, node_edges):\n        if node_id in self.graph:\n            self.graph[node_id]['edges'] = node_edges","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba139927ab212dde350481b6a02eb8d2210b3255","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":0,"height":4,"hidden":false,"row":290,"width":4},"report_default":{}}}}},"cell_type":"markdown","source":"### <font color=\"red\">7.2.1 Add the Project Nodes </font>  \n\nInitialize the project graph and iterate in project proflies to add the nodes with their properties"},{"metadata":{"_uuid":"6d0b2b3130360ef03d90282bf7937fcfe9beb26b","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"pg = ProjectsGraph(graph_name = 'projects')\n\nfor idx, row in projects_mini.iterrows():\n    node_id = row['Project ID']\n    node_properties = dict(row)\n    del node_properties['Project Essay']\n    del node_properties['Project Need Statement'] \n    del node_properties['Project Short Description']\n    pg._create_node(node_id, node_properties)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ce1ab33b544a88d9e3e917feaf60bc6c0ac359b","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":4,"height":4,"hidden":false,"row":290,"width":4},"report_default":{}}}}},"cell_type":"markdown","source":"Lets view a node "},{"metadata":{"_uuid":"ed839cb0f17ab268668a898015eb7ec35929ab84","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"pg._view_nodes()['83b4f3fbe743cb12ae2be7347ef03ecb']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ed9b90f59d0ebedecd65fadec5a143ed35d4b68","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":0,"height":4,"hidden":false,"row":294,"width":12},"report_default":{}}}}},"cell_type":"markdown","source":"### <font color=\"red\">7.2.2 Add the Edges among Project Nodes </font>  \n\nUsing interaction matrices created earlier, create the edges among project nodes "},{"metadata":{"_uuid":"c26491f65ac713d64ceb922a3f1afacbb17739f1","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"def get_similar_projects(project_id, num):\n    recs = project_edges[project_id][:num]\n    return recs \n\nfor idx, row in projects_mini.iterrows():\n    node_id = row['Project ID']\n    node_edges = get_similar_projects(project_id=node_id, num=5)\n    pg._create_edges(node_id, node_edges)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a0f2f787c60833f4383f373e00a23e664826abc","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":8,"height":4,"hidden":false,"row":290,"width":4},"report_default":{}}}}},"cell_type":"markdown","source":"Lets view the edges of a node"},{"metadata":{"_uuid":"dde9db16a0278807ae040814e155596627495de3","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"pg._view_nodes()['83b4f3fbe743cb12ae2be7347ef03ecb']['edges']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07be96981c7c1a292185807520fa5b04fa97e001","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":0,"height":4,"hidden":false,"row":298,"width":4},"report_default":{}}}}},"cell_type":"markdown","source":"Lets view the graphs\n\n### <font color=\"red\">7.2.2.1 Donors Graph</font>"},{"metadata":{"_kg_hide-input":true,"_uuid":"9375cd5e01f512a14ef1f18b3ba91e6a2be6ed26","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"from IPython.core.display import display, HTML, Javascript\nimport IPython.display\nimport json\n\nnodes = []\nlinks = []\n\nnodes.append({'id' : 'Donors', 'group' : 2, 'size' : 20 })\nfor key, val in list(dg._view_nodes().items())[:50]:\n    if len(val['edges']) == 0:\n        continue\n    nodes.append({'id' : key, 'group' : 2, 'size' : 15})\n    links.append({\"source\" : \"Donors\", \"target\" : key, \"value\" : 10})\n    \n    for node in val['edges']:\n        nodes.append({'id' : node[1], 'group' : 2, 'size' : 12})\n        \n        sv = np.log(node[0])\n        ew = 10\n        if sv > 6:\n            ew = 100\n        elif sv > 5:\n            ew = 20\n        elif sv > 4:\n            ew = 15\n        else:\n            ew = 10\n                    \n        links.append({\"source\": key, \"target\": node[1], \"value\": ew})\ndoc = {'nodes' : nodes, 'links' : links}\nwith open(\"donorg.json\", \"w\") as fout:\n    fout.write(json.dumps(doc))\n    \n    \n\nnodes = []\nlinks = []\nnodes.append({'id' : 'Projects', 'group' : 0, 'size' : 20, \"title\" : \"Projects\" })\nfor key, val in list(pg._view_nodes().items())[:75]:\n    if len(val['edges']) == 0:\n        continue\n\n    nodes.append({'id' : key, 'group' : 0, 'size' : 15})\n    links.append({\"source\" : \"Projects\",\"title\" : val['Project Title'], \"target\" : key, \"value\" : 10})\n    for node in val['edges']:\n        title = projects_mini[projects_mini['Project ID'] == node[1]]['Project Title'].iloc(0)[0]\n        nodes.append({'id' : node[1], 'group' : 2, 'size' : 12, \"title\": title})\n        links.append({\"source\": key, \"target\": node[1], \"value\": 8})\ndoc = {'nodes' : nodes, 'links' : links}\nwith open(\"projectg.json\", \"w\") as fout:\n    fout.write(json.dumps(doc))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"d77338fc1f9d3df662df8d4dee3a5d78c773fe63","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"html7=\"\"\"<!DOCTYPE html>\n<meta charset=\"utf-8\">\n<style>\n\n.links line {\n  stroke: #999;\n  stroke-opacity: 0.8;\n}\n.node text {\n  pointer-events: none;\n  font: 10px sans-serif;\n}\n\n.tooldiv {\n    display: inline-block;\n    width: 120px;\n    background-color: white;\n    color: #000;\n    text-align: center;\n    padding: 5px 0;\n    border-radius: 6px;\n    z-index: 1;\n}\n.nodes circle {\n  stroke: #fff;\n  stroke-width: 1.5px;\n}\n\ndiv.tooltip {\t\n    position: absolute;\t\t\t\n    text-align: center;\t\t\t\n    width: 250px;\t\t\t\t\t\n    height: 40px;\t\t\t\t\t\n    padding: 2px;\t\t\t\t\n    font: 12px sans-serif;\t\t\n    background: lightsteelblue;\t\n    border: 0px;\t\t\n    border-radius: 8px;\t\t\t\n    pointer-events: none;\t\t\t\n}\n\n</style>\n<svg id=\"donorg\" width=\"860\" height=\"760\"></svg>\"\"\"\n\njs7=\"\"\"require.config({\n    paths: {\n        d3: \"https://d3js.org/d3.v4.min\"\n     }\n });\n \nrequire([\"d3\"], function(d3) {// Dimensions of sunburst.\n \nvar svg = d3.select(\"#donorg\"),\n    width = +svg.attr(\"width\"),\n    height = +svg.attr(\"height\");\n\nvar color = d3.scaleOrdinal(d3.schemeCategory20);\n\nvar simulation = d3.forceSimulation()\n\n    .force(\"link\", d3.forceLink().id(function(d) { return d.id; }).distance(20).strength(1))\n    .force(\"charge\", d3.forceManyBody().strength(-155))\n    .force(\"center\", d3.forceCenter(width / 2, height / 2));\n\nd3.json(\"donorg.json\", function(error, graph) {\n  if (error) throw error;\n\n  var link = svg.append(\"g\")\n      .attr(\"class\", \"links\")\n    .selectAll(\"line\")\n    .data(graph.links)\n    .enter().append(\"line\")\n      .attr(\"stroke-width\", function(d) { return Math.sqrt(d.value); });\n\n// Define the div for the tooltip\nvar div = d3.select(\"body\").append(\"div\")\t\n    .attr(\"class\", \"tooltip\")\t\t\t\t\n    .style(\"opacity\", 0);\n\n  var node = svg.append(\"g\")\n      .attr(\"class\", \"nodes\")\n    .selectAll(\"circle\")\n    .data(graph.nodes)\n    .enter().append(\"circle\")\n      .attr(\"r\", function(d) {return d.size})\n      .attr(\"fill\", function(d) { return color(d.group); })\n      .call(d3.drag()\n          .on(\"start\", dragstarted)\n          .on(\"drag\", dragged)\n          .on(\"end\", dragended)).on(\"mouseover\", function(d) {\t\t\n            div.transition()\t\t\n                .duration(200)\t\t\n                .style(\"opacity\", .9);\t\t\n            div.html(d.id)\n                .style(\"left\", (d3.event.pageX) + \"px\")\t\t\n                .style(\"top\", (d3.event.pageY - 28) + \"px\");\t\n            })\t\t\t\t\t\n        .on(\"mouseout\", function(d) {\t\t\n            div.transition()\t\t\n                .duration(500)\t\t\n                .style(\"opacity\", 0);\t\n        });\n          \n    \n\n  simulation\n      .nodes(graph.nodes)\n      .on(\"tick\", ticked);\n      \n\n  simulation.force(\"link\")\n      .links(graph.links);\n\n  function ticked() {\n    link\n        .attr(\"x1\", function(d) { return d.source.x; })\n        .attr(\"y1\", function(d) { return d.source.y; })\n        .attr(\"x2\", function(d) { return d.target.x; })\n        .attr(\"y2\", function(d) { return d.target.y; });\n\n    node\n        .attr(\"cx\", function(d) { return d.x; })\n        .attr(\"cy\", function(d) { return d.y; });\n  }\n});\n\nfunction dragstarted(d) {\n  if (!d3.event.active) simulation.alphaTarget(0.3).restart();\n  d.fx = d.x;\n  d.fy = d.y;\n}\n\nfunction dragged(d) {\n  d.fx = d3.event.x;\n  d.fy = d3.event.y;\n}\n\nfunction dragended(d) {\n  if (!d3.event.active) simulation.alphaTarget(0);\n  d.fx = null;\n  d.fy = null;\n}\n });\n\"\"\"\n\nh = display(HTML(html7))\nj = IPython.display.Javascript(js7)\nIPython.display.display_javascript(j)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7acbb27da5869b1b0829b80f7ca0ecdeebe84f56","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":4,"height":4,"hidden":false,"row":298,"width":4},"report_default":{}}}}},"cell_type":"markdown","source":"### <font color=\"red\">7.2.2.2 Projects Graph </font>"},{"metadata":{"_kg_hide-input":true,"_uuid":"d7d2cab9e6d9e7ce6d8e3ff4bafe2ccc9a27c891","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"html8=\"\"\"<!DOCTYPE html>\n<meta charset=\"utf-8\">\n<style>\n\n.links line {\n  stroke: #999;\n  stroke-opacity: 0.8;\n}\n.node text {\n  pointer-events: none;\n  font: 10px sans-serif;\n}\n\n.tooldiv {\n    display: inline-block;\n    width: 120px;\n    background-color: white;\n    color: #000;\n    text-align: center;\n    padding: 5px 0;\n    border-radius: 6px;\n    z-index: 1;\n}\n.nodes circle {\n  stroke: #fff;\n  stroke-width: 1.5px;\n}\n\ndiv.tooltip {\t\n    position: absolute;\t\t\t\n    text-align: center;\t\t\t\n    width: 250px;\t\t\t\t\t\n    height: 40px;\t\t\t\t\t\n    padding: 2px;\t\t\t\t\n    font: 12px sans-serif;\t\t\n    background: lightsteelblue;\t\n    border: 0px;\t\t\n    border-radius: 8px;\t\t\t\n    pointer-events: none;\t\t\t\n}\n\n</style>\n<svg id=\"projectg\" width=\"860\" height=\"760\"></svg>\"\"\"\n\njs8=\"\"\"\n \nrequire([\"d3\"], function(d3) {// Dimensions of sunburst.\n \nvar svg = d3.select(\"#projectg\"),\n    width = +svg.attr(\"width\"),\n    height = +svg.attr(\"height\");\n\nvar color = d3.scaleOrdinal(d3.schemeCategory20);\n\nvar simulation = d3.forceSimulation()\n\n    .force(\"link\", d3.forceLink().id(function(d) { return d.id; }).distance(20).strength(1))\n    .force(\"charge\", d3.forceManyBody().strength(-155))\n    .force(\"center\", d3.forceCenter(width / 2, height / 2));\n\nd3.json(\"projectg.json\", function(error, graph) {\n  if (error) throw error;\n\n  var link = svg.append(\"g\")\n      .attr(\"class\", \"links\")\n    .selectAll(\"line\")\n    .data(graph.links)\n    .enter().append(\"line\")\n      .attr(\"stroke-width\", function(d) { return Math.sqrt(d.value); });\n\n// Define the div for the tooltip\nvar div = d3.select(\"body\").append(\"div\")\t\n    .attr(\"class\", \"tooltip\")\t\t\t\t\n    .style(\"opacity\", 0);\n\n  var node = svg.append(\"g\")\n      .attr(\"class\", \"nodes\")\n    .selectAll(\"circle\")\n    .data(graph.nodes)\n    .enter().append(\"circle\")\n      .attr(\"r\", function(d) {return d.size})\n      .attr(\"fill\", function(d) { return color(d.group); })\n      .call(d3.drag()\n          .on(\"start\", dragstarted)\n          .on(\"drag\", dragged)\n          .on(\"end\", dragended)).on(\"mouseover\", function(d) {\t\t\n            div.transition()\t\t\n                .duration(200)\t\t\n                .style(\"opacity\", .9);\t\t\n            div.html(d.title)\n                .style(\"left\", (d3.event.pageX) + \"px\")\t\t\n                .style(\"top\", (d3.event.pageY - 28) + \"px\");\t\n            })\t\t\t\t\t\n        .on(\"mouseout\", function(d) {\t\t\n            div.transition()\t\t\n                .duration(500)\t\t\n                .style(\"opacity\", 0);\t\n        });\n          \n    \n\n  simulation\n      .nodes(graph.nodes)\n      .on(\"tick\", ticked);\n      \n\n  simulation.force(\"link\")\n      .links(graph.links);\n\n  function ticked() {\n    link\n        .attr(\"x1\", function(d) { return d.source.x; })\n        .attr(\"y1\", function(d) { return d.source.y; })\n        .attr(\"x2\", function(d) { return d.target.x; })\n        .attr(\"y2\", function(d) { return d.target.y; });\n\n    node\n        .attr(\"cx\", function(d) { return d.x; })\n        .attr(\"cy\", function(d) { return d.y; });\n  }\n});\n\nfunction dragstarted(d) {\n  if (!d3.event.active) simulation.alphaTarget(0.3).restart();\n  d.fx = d.x;\n  d.fy = d.y;\n}\n\nfunction dragged(d) {\n  d.fx = d3.event.x;\n  d.fy = d3.event.y;\n}\n\nfunction dragended(d) {\n  if (!d3.event.active) simulation.alphaTarget(0);\n  d.fx = null;\n  d.fy = null;\n}\n });\n\"\"\"\n\nh = display(HTML(html8))\nj = IPython.display.Javascript(js8)\nIPython.display.display_javascript(j)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e08f71d0c48a590c6f55a1d3fcea0b4552e21be","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":0,"height":15,"hidden":false,"row":302,"width":12},"report_default":{}}}}},"cell_type":"markdown","source":"Note - The centeral nodes are added so that graph remains intact. Also, similar type of graphs can be created with different geographies. Example - Graphs for different states. \n\n# <font color=\"red\">8. Connecting Donors to Projects </font>\n\nIn this section, we will find the right donors of the projects using two methods - Graph Based Approach and Deep Neural Networks Approach. \n\n## <font color=\"red\">8.1 Graph based approach </font>\n\nThe main flow of generating recommendations using graphs is the following: \n\n>    1. Create the project vector for the new project. This project vector acts as a new node which can be added to the graphs in order to find its connections. \n>    2. To find the most relevant donors of this project, find the edges of this new project node from the donors graph. This will give the first set of recommended donors.  \n>    3. Extend the recommended donors set by Finding the most similar projects of this new node from the projects graph and obtaining their donors.  \n>    4. Also from the donors graph, Find the similar donors of all the donors obtained from donors.  \n>    5. Finally, Make use of non text features to filter out the recommended donors.  "},{"metadata":{"_uuid":"8efc99be11d9ff79b3666623edce3d260931b191","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"def connect_project_donors(project_id):\n        \n    # get the project index\n    proj_row = projects_mini[projects_mini['Project ID'] == project_id]\n    proj_ind = proj_row.index\n\n    # get the project vector\n    proj_vector = xtrain_embeddings[proj_ind]\n    \n    # match the vector with the user vectors \n    cossim_proj_user = linear_kernel(proj_vector, user_embeddings_matrix)\n    reverse_matrix = cossim_proj_user.T\n    reverse_matrix = np.array([x[0] for x in reverse_matrix])\n    similar_indices = reverse_matrix.argsort()[::-1]\n    \n    # filter the recommendations\n    projects_similarity = []\n    recommendations = []\n    top_users = [(reverse_matrix[i], user_profile['Donor ID'][i]) for i in similar_indices[:10]]\n    for x in top_users:\n        user_id = x[1]\n        user_row= user_profile[user_profile['Donor ID'] == user_id]\n        \n        ## to get the appropriate recommendations, filter them using other features \n        cat_count = 0\n        \n        ## Making use of Non Text Features to filter the recommendations\n        subject_categories = proj_row['Project Subject Category Tree'].iloc(0)[0]\n        for sub_cat in subject_categories.split(\",\"):\n            if sub_cat.strip() in user_row['Category_Map'].iloc(0)[0]:\n                cat_count += user_row['Category_Map'].iloc(0)[0][sub_cat.strip()]\n\n        grade_category = proj_row['Project Grade Level Category'].iloc(0)[0]\n        if grade_category in user_row['Category_Map'].iloc(0)[0]:\n            cat_count += user_row['Category_Map'].iloc(0)[0][grade_category]\n\n        metro_type = proj_row['School Metro Type'].iloc(0)[0]\n        if metro_type in user_row['SchoolMetroType_Map'].iloc(0)[0]:\n            cat_count += user_row['SchoolMetroType_Map'].iloc(0)[0][metro_type]\n        \n        x = list(x)\n        x.append(cat_count)\n        recommendations.append(x)\n        \n        ## Find similar donors\n        donor_nodes = dg._view_nodes()\n        if x[1] in donor_nodes:\n            recommendations.extend(donor_nodes[x[1]]['edges'])\n\n    ## Find Similar Projects \n    project_nodes = pg._view_nodes()\n    if project_id in project_nodes:\n        projects_similarity.extend(project_nodes[project_id]['edges'])    \n\n    return projects_similarity, recommendations\n    \ndef get_recommended_donors(project_id):\n    # Find the recommended donors and the similar projects for the given project ID \n    sim_projs, recommendations = connect_project_donors(project_id)\n\n    # filter the donors who have already donated in the project\n    current_donors = donations_df[donations_df['Project ID'] == project_id]['Donor ID'].tolist()\n\n    # Add the donors of similar projects in the recommendation\n    for simproj in sim_projs:\n        recommendations.extend(connect_project_donors(simproj[1])[1])\n    \n    ######## Create final recommended donors dataframe \n    # 1. Most relevant donors for a project \n    # 2. Similar donors of the relevant donors \n    # 3. Donors of the similar project \n    \n    recommended_df = pd.DataFrame()\n    recommended_df['Donors'] = [x[1] for x in recommendations]\n    recommended_df['Score'] = [x[0] for x in recommendations]\n    recommended_df = recommended_df.sort_values('Score', ascending = False)\n    recommended_df = recommended_df.drop_duplicates()\n\n    recommended_df = recommended_df[~recommended_df['Donors'].isin(current_donors)]\n    return recommended_df\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a09ade8f1da7c5034bc0a2cd777ba2f22861c46","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":8,"height":4,"hidden":false,"row":298,"width":4},"report_default":{}}}}},"cell_type":"markdown","source":"**Note -** In the above implementation, I used non text features to filter the recommendations, these can be used again according to the use-case. One can define what sort of filters they need to apply on the recommendations set.  \n\n### Lets view some results for the given projects "},{"metadata":{"_uuid":"3ca7f9adab824bcdc13026f3ac9a22a3537822cb","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"def _get_results(project_id):\n    proj = projects_mini[projects_mini['Project ID'] == project_id]\n    print (\"Project ID: \" + project_id )\n    print (\"Project Title: \" + proj['Project Title'].iloc(0)[0])\n    print (\"\")\n\n    print (\"Recommended Donors: \")\n    recs = get_recommended_donors(project_id)\n    donated_projects = []\n    for i, row in recs.head(10).iterrows():\n        donor_id = row['Donors']\n        print (donor_id +\" | \"+ str(row['Score']))\n        donor_projs = user_profile[user_profile['Donor ID'] == donor_id]['Project Title']['<lambda>'].iloc(0)[0]\n        donor_projs = donor_projs.split(\",\")\n        for donor_proj in donor_projs:\n            if donor_proj not in donated_projects:\n                donated_projects.append(donor_proj)\n    print (\"\")\n    print (\"Previous Projects of the Recommended Donors: \")\n    for proj in donated_projects:\n        print (\"-> \" + proj)\n\nproject_id = \"d20ec20b8e4de165476dfd0a68da0072\"\n_get_results(project_id)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a56e1208c9c8297ed22ef221e088a5d4d7817c48","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"project_id = \"ad51c22f0d31c7dc3103294bdd7fc9c1\"\n_get_results(project_id)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"522ff4e749290b17858ba1ed05b0ac918e840d3a","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":0,"height":79,"hidden":false,"row":317,"width":12},"report_default":{}}}}},"cell_type":"markdown","source":"So these were the recommendations which mainly used the content features of the projects and donors in order to establish right relationships among the two verticals.\n\n# <font color=\"red\">9.2 Connecting Donors - Projects using DeepNets</font>\n\n\"Donors who donated in these projects also donated in these other projects.\" \n\nIn the previous part, A graph based content similarity matching techniques were used to match the donors for the projects. In this section, we will use a deep neural network to identify the donors having similar behaviours. This approach is very close to collaborative filtering approach. \n\n> **Question** - Are there any particular donors you want to target through this competition. Like first time donors or repeat donors or both?\n\n> **Thomas Vo (from donorschoose.org)** - The short answer is no, we're not targeting a specific subset. This is because they are both important to our business, they behave very differently, and we'd like to improve our odds for both subsets. I will say though, that whereas we have systems in place to accommodate repeat donors, we are really not sure what to do with new donors. It's a sample size of one and the projects are ephemeral, which makes it especially difficult to figure out which projects would entice these donors.\n\n\nWhile it is true that this approach may not work very well for first time donors but it can work very well for the donors having multiple donation. One way to ensemble this approach for first time donors can be to find the similarities among the donors based on content and context which belong to the first time donors set. Following figure depicts how this approach works.\n\n<br>\n\n![deepcolab.png](https://i.imgur.com/PpopDBB.png)\n\n<br>\n\n\nThe main idea behind this approach is to identify donors based on the similarity in their behaviours. In the donorschoose usecase the behavious can be indirectly represented by the donation amounts from donors in the projects in which they have donated in the past. \n\nGenerating outcomes using DeepNets can be thought of as a regression problem where the model tries to predict the interaction scores for each project for each donor. The donors having high interaction scores with respect to a project are treated as the recommendations. In this approach, the model learns the donor and project embeddings as the features and predicts what could be the outcome of a donor - project combination. \n\n\n### Process Flow of Deep Net Model\n\n- In the deepnets model, the embedding vectors (different from word embeddings) for the donors and one for the projects are created. \n- These embedding vectors are merged together to give a single vector which repersents the latent infromation about a donor-project pair. \n\nPlease note that these embeddings are different from the word embeddings. These embeddings are a repersentation of the behaviour of the donor with multiple projects and the interactions of multiple donors on a single project. For example, as shown  \n\n![donor-pro.png](https://i.imgur.com/1WOmUBk.png)\n\n- The merged vector becomes the independent variable of the dataset. The target variable of this dataset is the logarithmic of donation amount or interaction score \n\n![DeepNEt.png](https://i.imgur.com/ogdVSaj.jpg)\n\n- During the training process, the embeddings parameters are learned which gives a latent representation of the donors / projects.  \n- The learned model is used to predict if a new donor-project combination has higher interaction score. If it is higher than the donor is reocmmended for the project. \n\n### References used \n\n1. [Neural Item Embedding For Collaborative Filtering] https://arxiv.org/pdf/1603.04259.pdf\n2. [Neural Collaborative Filtering] https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf\n3. [Fast.ai] http://www.fast.ai/\n\n\n## <font color=\"red\">9.2.1 Creating the Donor-Project-Interaction Matrix</font>\n\nCreate the interaction data frame - Project, Donors, InteractionScore2"},{"metadata":{"_uuid":"93529fbeea5de400ff46d306365052d43a30f556","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"## create the interaction data frames\ninteractions = users[['Project ID', 'Donor ID', 'Donation Amount']]\ninteractions['interaction_score_2'] = np.log(interactions['Donation Amount'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"909f84c50fafbf1a07ad818c2cb934b461f12558","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":0,"height":4,"hidden":false,"row":396,"width":12},"report_default":{}}}}},"cell_type":"markdown","source":"## <font color=\"red\">9.2.2 PreProcessing</font>\n\nFor the model implementation in Keras, I mapped all the donor ids and project ids to an integer between 0 and either the total number of donors or the total number of projects."},{"metadata":{"_uuid":"dbb3093d90e8e00900d7691fecca58af0523035f","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"unique_donors = list(interactions['Donor ID'].value_counts().index)\ndonor_map = {}\nfor i, user in enumerate(unique_donors):\n    donor_map[user] = i+1\n\nunique_projs = list(interactions['Project ID'].value_counts().index)\nproj_map = {}\nfor i, proj in enumerate(unique_projs):\n    proj_map[proj] = i+1\n\ntags = {'donors' : donor_map, 'projects' : proj_map}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0aaf96301ae34193e4b2ab92356814e43155e9b8","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":0,"height":4,"hidden":false,"row":400,"width":4},"report_default":{}}}}},"cell_type":"markdown","source":"Once the Id maps have been created, now convert the actual project and donor ids in the integer forms"},{"metadata":{"_uuid":"7aad6504eb27e46fdf6272624f517e031b8f75c1","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"def getID(val, tag):\n    return tags[tag][val]\n     \ninteractions['proj_id'] = interactions['Donor ID'].apply(lambda x : getID(x, 'donors'))\ninteractions['user_id'] = interactions['Project ID'].apply(lambda x : getID(x, 'projects'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88ea7c10e565b0a7f194eecb332fd6c2a47d869e","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":4,"height":4,"hidden":false,"row":400,"width":4},"report_default":{}}}}},"cell_type":"markdown","source":"Obtain the maximum number of donors and projects present in the dataset. This will be used to define the network architecture"},{"metadata":{"_uuid":"9433a4ffdadb85947c5510806778f26d3f128eb7","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"# remove the duplicate entries in the dataset \nmax_userid = interactions['user_id'].drop_duplicates().max() + 1\nmax_movieid = interactions['proj_id'].drop_duplicates().max() + 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"39e866209f5d03739998e30f05c74a494e249ec7","collapsed":true,"extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":8,"height":4,"hidden":false,"row":400,"width":4},"report_default":{}}}}},"cell_type":"markdown","source":"Make a random shuffling on the dataset"},{"metadata":{"_uuid":"0e55003c2ac1dbbd0878c04e5458d9f992df26b7","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"shuffled_interactions = interactions.sample(frac=1., random_state=153)\nPastDonors = shuffled_interactions['user_id'].values\nPastProjects = shuffled_interactions['proj_id'].values\nInteractions = shuffled_interactions['interaction_score_2'].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"721cbc70afade317747abb99958bb737f5935337","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":0,"height":7,"hidden":false,"row":404,"width":12},"report_default":{}}}}},"cell_type":"markdown","source":"## <font color=\"red\">9.2.3 Create Model Architecture</font>\n\nLets create the model architecture. As discussed in the above sections, the deep neural network learns the embeddings of donors and projects.  \n\nThere are essentially three parts of this model.  \n\n1. Donor Embedding Architecture  \n2. Project Embedding Architecture  \n3. Multi Layer Perceptron Architecture   \n"},{"metadata":{"_uuid":"d5a8078cb71a723a39ee96f3a745807369d40715","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"def create_model(n_donors, m_projects, embedding_size):\n    \n    # add input layers for donors and projects\n    donor_id_input = Input(shape=[1], name='donor')\n    project_id_input = Input(shape=[1], name='project')\n\n    # create donor and project embedding layers \n    donor_embedding = Embedding(output_dim=embedding_size, input_dim=n_donors,\n                               input_length=1, name='donor_embedding')(donor_id_input)\n    project_embedding = Embedding(output_dim=embedding_size, input_dim=m_projects,\n                               input_length=1, name='project_embedding')(project_id_input)\n    \n    # perform reshaping on donor and project vectors \n    donor_vecs = Reshape([embedding_size])(donor_embedding)\n    project_vecs = Reshape([embedding_size])(project_embedding)\n    \n    # concatenate the donor and project embedding vectors \n    input_vecs = Concatenate()([donor_vecs, project_vecs])\n    \n    # add a dense layer\n    x = Dense(128, activation='relu')(input_vecs)\n    \n    # add the output layer\n    y = Dense(1)(x)\n    \n    # create the model using inputs and outputs \n    model = Model(inputs=[donor_id_input, project_id_input], outputs=y)\n    \n    # compile the model, add optimizer function and loss function\n    model.compile(optimizer='adam', loss='mse')  \n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75efb26019fb0c85a4358c18a07405173056ec4d","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":0,"height":4,"hidden":false,"row":411,"width":4},"report_default":{}}}}},"cell_type":"markdown","source":"Obtain the model"},{"metadata":{"_uuid":"816157fd9728e72d609a3e27d14ffca42837c21f","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"embedding_size = 10\nmodel = create_model(max_userid, max_movieid, embedding_size)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7bad2657a9f337ad168c73357370b8b2a8bb9a41","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":4,"height":4,"hidden":false,"row":411,"width":4},"report_default":{}}}}},"cell_type":"markdown","source":"Lets view the model summary"},{"metadata":{"_uuid":"d82ca58c767a8121031c219d96b05bab8cbe9a4f","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b26e8dd90cb2ac3209f6f84af257418d9aa380db","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":8,"height":4,"hidden":false,"row":411,"width":4},"report_default":{}}}}},"cell_type":"markdown","source":"Create a function which can be used to generate the interaction scores "},{"metadata":{"_uuid":"9a58d0a5ce4d16bc332c3ef6866122d36bbc2192","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"def rate(model, user_id, item_id):\n    return model.predict([np.array([user_id]), np.array([item_id])])[0][0]\n\ndef predict_rating(movieid, userid):\n    return rate(model, movieid - 1, userid - 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7aa0d8f8b2cc19f9e9157ed3a0cb367fadf78f4","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":0,"height":4,"hidden":false,"row":415,"width":4},"report_default":{}}}}},"cell_type":"markdown","source":"## <font color=\"red\">9.2.4 Train the model </font>\n\nTrain the model with early stopping callback"},{"metadata":{"_uuid":"a3b3acdd2bc77b69f44297e7bf36c5ebb58844a5","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"## with more data, nb_epooch can also be increased\nhistory = model.fit([PastDonors, PastProjects], Interactions, nb_epoch=2, validation_split=.20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0679ae921713844f7b9d612eef42151394711445","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"min_val_loss, idx = min((val, idx) for (idx, val) in enumerate(history.history['val_loss']))\nprint ('Minimum RMSE at epoch', '{:d}'.format(idx+1), '=', '{:.4f}'.format(math.sqrt(min_val_loss)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"92a1a4361c0a339f0062f2b210f31a75a842ef68","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":0,"height":4,"hidden":false,"row":419,"width":12},"report_default":{}}}}},"cell_type":"markdown","source":"## <font color=\"red\">9.2.5 Generate the predictions for a project </font>\n\nGenerate the predictions, before that perform some basic preprocessing."},{"metadata":{"_uuid":"f18981f58783d25a2ab1d82a4b93eda7a02eb584","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"Past_Donors = users[['Donor ID', 'Donor City']]\nPast_Projects = users[['Project ID', 'Project Title']]\n\nPast_Donors = Past_Donors.drop_duplicates()\nPast_Projects = Past_Projects.drop_duplicates()\n\nPast_Donors['user_id'] = Past_Donors['Donor ID'].apply(lambda x : getID(x, 'donors'))\nPast_Projects['proj_id'] = Past_Projects['Project ID'].apply(lambda x : getID(x, 'projects'))\n\n## for this sample run, get common IDs from content based and \n## collaborative approaches to get the results together\n\nlist1 = list(projects_mini['Project ID'].values)\nlist2 = list(Past_Projects['Project ID'].values)\ncommon_ids = list(set(list1).intersection(list2))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64a67dca3f64e0b5b7a87521b30a8d0307407291","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":4,"height":4,"hidden":false,"row":415,"width":4},"report_default":{}}}}},"cell_type":"markdown","source":"### <font color=\"red\">Sample Predictions - 1</font>"},{"metadata":{"_uuid":"20b12bfb5c5fe4240638fe8eb168b8e6f618eef3","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}},"trusted":true,"collapsed":true},"cell_type":"code","source":"idd = proj_map['ad51c22f0d31c7dc3103294bdd7fc9c1']\n\nuser_ratings = interactions[interactions['proj_id'] == idd][['user_id', 'proj_id', 'interaction_score_2']]\nuser_ratings['predicted_amt'] = user_ratings.apply(lambda x: predict_rating(idd, x['user_id']), axis=1)\nrecommendations = interactions[interactions['user_id'].isin(user_ratings['user_id']) == False][['user_id']].drop_duplicates()\nrecommendations['predicted_amt'] = recommendations.apply(lambda x: predict_rating(idd, x['user_id']), axis=1)\nrecommendations['predicted_amt'] = np.exp(recommendations['predicted_amt'])\nrecommendations.sort_values(by='predicted_amt', ascending=False).merge(Past_Donors, on='user_id', how='inner', suffixes=['_u', '_m']).head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b40d866d35203e06a7b70ccee4b0360b0a45907a"},"cell_type":"markdown","source":"### <font color=\"red\">Final Results : combine the results of two approaches</font>"},{"metadata":{"_kg_hide-input":true,"_uuid":"f7277988c26e8b820158879f0a4482855d0c56c6","trusted":true,"collapsed":true},"cell_type":"code","source":"project_id = \"ad51c22f0d31c7dc3103294bdd7fc9c1\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c19ab2ada7f146648d9829216ff80942e428ce49"},"cell_type":"markdown","source":"#### <font color=\"red\">1. Recommendations using Content+Context Similarity</font>"},{"metadata":{"_kg_hide-input":true,"_uuid":"517d9fb532dbc69e92518dfafc4bc0dbd41abd45","trusted":true,"collapsed":true},"cell_type":"code","source":"_get_results(project_id)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"702e737480760b5785b6470a29b1141380f84e09"},"cell_type":"markdown","source":"#### <font color=\"red\">2. Recommendations using Behavioural Similarity</font>"},{"metadata":{"_kg_hide-input":true,"_uuid":"3ba2b5616b80a5addfb199f9652b0117de779445","trusted":true,"collapsed":true},"cell_type":"code","source":"title = projects_mini[projects_mini['Project ID'] == project_id]['Project Title'].iloc(0)[0]\nprint (\"Project ID: \" + project_id )\nprint (\"Project Title: \" + title)\nprint (\"\")\n    \nidd = proj_map[project_id]\nuser_ratings = interactions[interactions['proj_id'] == idd][['user_id', 'proj_id', 'interaction_score_2']]\nuser_ratings['predicted_amt'] = user_ratings.apply(lambda x: predict_rating(idd, x['user_id']), axis=1)\nrecommendations = interactions[interactions['user_id'].isin(user_ratings['user_id']) == False][['user_id']].drop_duplicates()\nrecommendations['predicted_amt'] = recommendations.apply(lambda x: predict_rating(idd, x['user_id']), axis=1)\nrecommendations['predicted_amt'] = np.exp(recommendations['predicted_amt'])\nrecs = recommendations.sort_values(by='predicted_amt', ascending=False).merge(Past_Donors, on='user_id', how='inner', suffixes=['_u', '_m']).head(10)\n\npast_projs = []\nprint (\"Donors based on Behavioural Similarity: \")\nfor i, row in recs.head(5).iterrows():\n    print (row['Donor ID'] +\" | Donated Amount: \"+ str(row['predicted_amt']))\n    dons = donations_df[donations_df['Donor ID'] == row['Donor ID']]\n    for i,x in dons.head(3).iterrows():\n        projs = projects_df[projects_df['Project ID'] == x['Project ID']]\n        title = projs['Project Title'].iloc(0)[0]\n        cost = projs['Project Cost'].iloc(0)[0]\n        txt =title + \" ($\" + str(cost) + \")\"\n        past_projs.append(txt)\n\nprint (\"\")\nprint (\"Past Projects and Donations: \")\nfor proj_ in past_projs:\n    print (proj_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e9e28633b8a21c313173c289bc5667f08334f11b","collapsed":true,"extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"hidden":true},"report_default":{}}}}},"cell_type":"markdown","source":"So in the final results we can see that donors are recommended from different characteristics. \n1. Donros whose features are similar to the given project are recommended. \n2. Donors of the projects which are contextually similar to a given project (for example - **Diverse Books for Dynamic Students**, **Thesaurus' Expand Vocabulary For English Language Learners!**, and **Teaching My Special Friends How To Communicate!** are contextually similar to **Native American Book Project**) are recommended. \n3. Donors having similar donors features are recommended.  \n4. Donors having similar behaviours are recommended using their past interactions on projects.  "},{"metadata":{"_uuid":"11b0ac4a0bdab3eae35963d58f82c66fbeb1f9c9","extensions":{"jupyter_dashboards":{"version":1,"views":{"grid_default":{"col":0,"height":5,"hidden":false,"row":423,"width":12},"report_default":{}}}}},"cell_type":"markdown","source":"### <font color=\"red\">Evaluation Metrics </font>  \n\nAfter deployment of this complete model, several evaluation metrics can be used to check if the model is making significant changes or not. Following are the differet ideas: \n\n1. Quality of Recommendations  - It is often helpful to manually evaluate if the generated outcomes align with the problem statement and the business logic. By looking at the quality of recommendations, one can check how good the recommendation model is working.  \n2. Marketing Metrics   \n    2.1 Open Rates : How many donors opened the email.  \n    2.2 Clickthrough Rate : Total number of unique clicks per 100 delivered emails  \n    2.3 Unsubscrive Rate : How many donors unsubscribed after receiving the emails.  \n    2.4 Number of Donors making the donations after receiving the emails.  \n3. Model Metrics  \n    3.1 Root Mean Squared Error : In the Deep Net which was used for generating the predicted donation amount for a given project - donor combincation, RMSE can help to evaluate the model. As in my plementation, we saw that after t epoochs the model RMSE was equal to 0.5. This can be further improved using more data. \n    \n### <font color=\"red\">End Notes</font>\n\nSo in this kernel I implemented different ideas to tackle the problem statement. I took different ideas from a number of research papers,  articles and my thought process.  \n\nAlso, it is important to note that this kernel mainly showcases \"The Hybrid approach to connect donors to projects\" and how to implement it. The results / accuracies of the output might not seem decent in some examples because it is not run on entire dataset.  However, if entire dataset of donors, projects, and external data is taken and the models are trained with more number of epoochs the results will improve further. \n\nThe references are given below: \n\n1. Content Based Recommendations (http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.130.8327&rep=rep1&type=pdf)\n2. Graph Based Collaborative Ranking (http://www.ra.ethz.ch/cdstore/www10/papers/pdf/p519.pdf)  \n3. Text Similarity with Word Embeddings (https://staff.fnwi.uva.nl/m.derijke/wp-content/papercite-data/pdf/kenter-short-2015.pdf)\n4. Word Embeddings for Text Similarity (http://ad-publications.informatik.uni-freiburg.de/theses/Bachelor_Jon_Ezeiza_2017.pdf)  \n5. Context Aware Recommendations Overview (https://www.slideshare.net/irecsys/contextaware-recommendation-a-quick-view)\n5. Neural Collaborative Filtering (https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf)  \n6. My Previous Research Paper on - Topic Modelling Driven Content Based Recommendation Engine (https://www.sciencedirect.com/science/article/pii/S1877050917326960)  \n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0352e4039522915d8cbdca679741c428883c4af8"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e23ea3f0253d11743d74f87b79a5c25a5fbf400b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"extensions":{"jupyter_dashboards":{"activeView":"grid_default","version":1,"views":{"grid_default":{"cellMargin":10,"defaultCellHeight":20,"maxColumns":12,"name":"grid","type":"grid"},"report_default":{"name":"report","type":"report"}}}},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}